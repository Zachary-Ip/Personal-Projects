{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tInjwFwaszkK"
   },
   "source": [
    "Until now we have been training the model with png spectrograms, which are colour images and therefor have 3 channels. The colour is just a color map, it doesn't actually mean anything, so we might as well train with the raw spectrogram values. \n",
    "\n",
    "I calculated the spectrograms and saved them all as .npy files, so I'll need to figure out how to get those into the data loader, then change the input channels to 1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NQz2PJsAw-TN"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import os\n",
    "import glob\n",
    "import imageio\n",
    "import random, shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8b6e80719172>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"!!! Failed to download data !!!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[1;31m# Redirect resolving generator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[1;31m# Redirect resolving generator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m                 resp = self.send(\n\u001b[0m\u001b[0;32m    238\u001b[0m                     \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mcontent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    829\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    751\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stream'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mstream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m                 if (\n\u001b[0;32m    521\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fname = \"music.zip\"\n",
    "url = \"https://osf.io/drjhb/download\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.ConnectionError:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "        with open(fname, \"wb\") as fid:\n",
    "            fid.write(r.content)\n",
    "            \n",
    "os.chdir('C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\Music')\n",
    "\n",
    "with ZipFile(fname, 'r') as zipObj:\n",
    "# Extract all the contents of zip file in different directory\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "DkaJYigCxBAr"
   },
   "outputs": [],
   "source": [
    "#Utils\n",
    "\n",
    "def set_device():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"WARNING: For this notebook to perform best, \"\n",
    "          \"if possible, in the menu under `Runtime` -> \"\n",
    "          \"`Change runtime type.`  select `GPU` \")\n",
    "    else:\n",
    "        print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "#  Plotting function.\n",
    "\n",
    "def plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc):\n",
    "    epochs = len(train_loss)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.plot(list(range(epochs)), train_loss, label='Training Loss')\n",
    "    ax1.plot(list(range(epochs)), validation_loss, label='Validation Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Epoch vs Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(list(range(epochs)), train_acc, label='Training Accuracy')\n",
    "    ax2.plot(list(range(epochs)), validation_acc, label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Epoch vs Accuracy')\n",
    "    ax2.legend()\n",
    "    fig.set_size_inches(15.5, 5.5)\n",
    "    plt.show()\n",
    "    \n",
    "def apply_sliding_window(in_dir, out_dir, window_length = 3, hop_length = 1, \n",
    "                         save_as_tensor = False):\n",
    "  #IMPORTANT NOTE - this function assumes that the spectrograms were made with \n",
    "    #default librosa nfft_size, hop_length, etc. \n",
    "    #window and hop length units are in seconds. \n",
    "    window_size = librosa.time_to_frames(window_length)\n",
    "    hop_size = librosa.time_to_frames(hop_length)\n",
    "    \n",
    "    #get list of genre folders\n",
    "    genre_folders = glob.glob(in_dir + \"*\")\n",
    "    #loop over genre folders\n",
    "    for g in genre_folders:\n",
    "        spec_files = glob.glob(g + \"\\\\*.npy\")\n",
    "    \n",
    "        out_folder = out_dir + os.path.basename(g) + \"\\\\\"\n",
    "        # Make sure save directory exists\n",
    "        if not os.path.exists(out_folder):\n",
    "            os.mkdir(out_folder)\n",
    "        \n",
    "        #loop over spectrogram files\n",
    "        for f in spec_files:\n",
    "            #extract file name\n",
    "            file_name = f.split('/')[-1]\n",
    "            \n",
    "            #load spectrogram\n",
    "            spec = np.load(file_name)\n",
    "            \n",
    "            save_name = file_name.strip(\".npy\")\n",
    "            \n",
    "            #apply sliding frame to spectrogram\n",
    "            all_frames = librosa.util.frame(spec, window_size, hop_size)\n",
    "            all_frames = np.moveaxis(all_frames, 2, 0)\n",
    "\n",
    "            #loop over individual frames\n",
    "            for i, frame in enumerate(all_frames):\n",
    "        \n",
    "                #specify out file name\n",
    "                    if save_as_tensor: #convert np array to tensor\n",
    "                        torch_frame = torch.from_numpy(frame)\n",
    "                        #save torch of spectrogram frame\n",
    "                        torch.save(torch_frame, out_folder + os.path.basename(save_name) + \"_\" + str(i) + \".pt\") \n",
    "                    else: #save spectrogram frame\n",
    "                        np.save(out_folder + os.path.basename(save_name) + \"_\" + str(i) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is enabled in this notebook.\n"
     ]
    }
   ],
   "source": [
    "device = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE4Xaq8YvIGY"
   },
   "source": [
    "## Make Mel Spectrograms with Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5SPsfpLvLtX",
    "outputId": "945936c7-c2b9-47b4-9e47-8f4d54a80df3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loop\n",
      "blues\n",
      "classical\n",
      "country\n",
      "disco\n",
      "hiphop\n",
      "jazz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipzac\\anaconda3\\lib\\site-packages\\librosa\\core\\audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something is wrong with jazz.00054 so it has been skipped\n",
      "metal\n",
      "pop\n",
      "reggae\n",
      "rock\n"
     ]
    }
   ],
   "source": [
    "in_folder_path = \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\genres_original\\\\\"\n",
    "out_folder_path = \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\spectrograms_30s\\\\\"\n",
    "if not os.path.exists(out_folder_path):\n",
    "    os.mkdir(out_folder_path)\n",
    "\n",
    "#get list of genre folders\n",
    "genre_folders = os.listdir(in_folder_path)\n",
    "\n",
    "#initialize empty list of problem files:\n",
    "problem_files = []\n",
    "\n",
    "#loop over genre folders\n",
    "print('starting loop')\n",
    "for genre_folder in genre_folders: \n",
    "    #get list of individual .wav files\n",
    "    os.chdir(in_folder_path + '\\\\' + genre_folder)\n",
    "    wav_files = os.listdir(\".\")\n",
    "  #get output folder path\n",
    "    out_folder = out_folder_path + '\\\\' + genre_folder\n",
    "  #make output folder if it doesn't exist\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.mkdir(out_folder)\n",
    "\n",
    "  #loop over list of wav files\n",
    "    print(genre_folder)\n",
    "    \n",
    "    for wav_file in wav_files:\n",
    "    #extract file name\n",
    "        file_name = wav_file.split('/')[-1]\n",
    "        file_name = file_name.strip(\".wav\")\n",
    "        try:\n",
    "            #load wav file\n",
    "            y, sr = librosa.load(wav_file)\n",
    "\n",
    "            #normalize audio\n",
    "            y = (y - y.mean())/ y.std()\n",
    "\n",
    "          #calculate mel spectrogram\n",
    "            spectrogram = librosa.feature.melspectrogram(y, sr = sr)\n",
    "            spectrogram_db = librosa.amplitude_to_db(spectrogram, ref = np.max)\n",
    "\n",
    "      #global normalization of the spectrogram (not frequency band normalization...)\n",
    "            spectrogram_norm = (spectrogram_db - spectrogram_db.mean()) / spectrogram_db.std()\n",
    "\n",
    "      #save spectrogram\n",
    "    \n",
    "            np.save(out_folder + '\\\\' +file_name, spectrogram_norm)\n",
    "        except:\n",
    "            print(\"Something is wrong with \" + file_name + \" so it has been skipped\")\n",
    "            problem_files = problem_files.append(wav_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNq0YxYpyeZM"
   },
   "source": [
    "## Split training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vCeMHisnyV3Y"
   },
   "outputs": [],
   "source": [
    "## EDIT HERE TO CHANGE SIZE OF TEST AND VAL DATASETS\n",
    "test_prop = 0.15\n",
    "val_prop = 0.15\n",
    "\n",
    "train_prop = 1 - test_prop - val_prop\n",
    "\n",
    "# Create folder with training, testing and validation data.\n",
    "\n",
    "spectrograms_dir = \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\spectrograms_30s\"\n",
    "folder_names = [\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train\\\\\", \n",
    "                \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\", \n",
    "                \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\\"]\n",
    "\n",
    "genres = list(os.listdir(spectrograms_dir))\n",
    "\n",
    "train_dir = folder_names[0]\n",
    "test_dir = folder_names[1]\n",
    "val_dir = folder_names[2]\n",
    "\n",
    "for f in folder_names:\n",
    "    if os.path.exists(f):\n",
    "        shutil.rmtree(f)\n",
    "        os.mkdir(f)\n",
    "    else:\n",
    "        os.mkdir(f)\n",
    "# Loop over all genres\n",
    "for g in genres:\n",
    "    if os.path.exists(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train\\\\\" + g + '\\\\' ):\n",
    "        shutil.rmtree(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train\\\\\" + g + '\\\\' )\n",
    "        os.mkdir(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train\\\\\" + g + '\\\\' )\n",
    "    else:\n",
    "        os.mkdir(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train\\\\\" + g + '\\\\' )\n",
    "        \n",
    "    if os.path.exists(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\" + g + '\\\\' ):\n",
    "        shutil.rmtree(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\" + g + '\\\\' )\n",
    "        os.mkdir(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\" + g + '\\\\' )\n",
    "    else:\n",
    "        os.mkdir(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\" + g + '\\\\' )\n",
    "        \n",
    "    if os.path.exists(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\\" + g + '\\\\' ):\n",
    "        shutil.rmtree(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\\" + g + '\\\\' )\n",
    "        os.mkdir(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\\" + g + '\\\\' )\n",
    "    else:\n",
    "        os.mkdir(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\\" + g + '\\\\' )\n",
    "        \n",
    "    src_file_paths= glob.glob('C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\spectrograms_30s\\\\' + g + '\\\\*')\n",
    "    # find all images & split in train, test, and validation\n",
    "    # random.shuffle(src_file_paths)\n",
    "\n",
    "    test_idx = int(len(src_file_paths) * test_prop)\n",
    "    val_idx = test_idx + int(len(src_file_paths) * val_prop)\n",
    "\n",
    "    test_files = src_file_paths[0:test_idx]\n",
    "    val_files = src_file_paths[test_idx:val_idx]\n",
    "    train_files = src_file_paths[val_idx:]\n",
    "\n",
    "  # copy training and testing images over\n",
    "    for f in train_files:\n",
    "        try:\n",
    "            shutil.copy(f, \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train\\\\\" + g + '\\\\' + os.path.split(f)[-1])\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "    for f in val_files:\n",
    "        try:\n",
    "            shutil.copy(f, \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\\" + g  + '\\\\' + os.path.split(f)[-1])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    for f in test_files:\n",
    "        try:\n",
    "            shutil.copy(f, \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\" + g  + '\\\\' + os.path.split(f)[-1])\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZ5eGK1K1wns"
   },
   "source": [
    "## Apply sliding window to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oObuS8PsG6sy",
    "outputId": "d3a067f3-52d2-426b-b155-2ba8535f36a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipzac\\anaconda3\\lib\\site-packages\\librosa\\util\\utils.py:198: UserWarning: librosa.util.frame called with axis=-1 on a non-contiguous input. This will result in a copy.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cropped_names = [\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train_cropped\\\\\", \n",
    "                \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test_cropped\\\\\", \n",
    "                \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val_cropped\\\\\"]\n",
    "for f in cropped_names:\n",
    "    if os.path.exists(f):\n",
    "        shutil.rmtree(f)\n",
    "        os.mkdir(f)\n",
    "    else:\n",
    "        os.mkdir(f)\n",
    "        \n",
    "# Cut Data\n",
    "\n",
    "apply_sliding_window( \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train\\\\\", \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train_cropped\\\\\", save_as_tensor=True)\n",
    "apply_sliding_window(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\", \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test_cropped\\\\\", save_as_tensor=True)\n",
    "apply_sliding_window(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\\", \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val_cropped\\\\\", save_as_tensor = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvnqIKVNmTrq"
   },
   "source": [
    "## Dataloader - torch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Lmq8RSpJzPn4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data loading.\n",
    "train_dir = \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train_cropped\\\\\"\n",
    "val_dir =  \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val_cropped\\\\\"\n",
    "\n",
    "train_dataset = datasets.DatasetFolder(\n",
    "    train_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load, \n",
    "    transform = transforms.Compose([torchaudio.transforms.TimeMasking(time_mask_param= 40, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param=40, iid_masks=True), \n",
    "                                    torchaudio.transforms.TimeMasking(time_mask_param= 30, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param = 30, iid_masks=True), \n",
    "                                    torchaudio.transforms.TimeMasking(time_mask_param= 20, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param = 20, iid_masks=True)]) )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)\n",
    "\n",
    "val_dataset = datasets.DatasetFolder(\n",
    "    val_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AgdXD7RSorg"
   },
   "source": [
    "## Train Default Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LD-455iX5AxJ"
   },
   "outputs": [],
   "source": [
    "# Make a CNN & train it to predict genres.\n",
    "\n",
    "class music_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Intitalize neural net layers\"\"\"\n",
    "        super(music_net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=0) # changed in channels to 1 \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=512, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv4 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(in_features=512, out_features=10) \n",
    "        \n",
    "        self.batchnorm1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(num_features=128)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(num_features=512)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(num_features=256)\n",
    "        self.batchnorm5 = nn.BatchNorm2d(num_features=128)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5, inplace=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "    # Conv layer 1.\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 2.\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 3.\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 4.\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # # Conv layer 5.\n",
    "        x = self.conv5(x)\n",
    "        x = self.batchnorm5(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Fully connected layer 1.\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, validation_loader, epochs):\n",
    "    criterion =  nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0025)\n",
    "    train_loss, validation_loss = [], []\n",
    "    train_acc, validation_acc = [], []\n",
    "    with tqdm(range(epochs), unit='epoch') as tepochs:\n",
    "        tepochs.set_description('Training')\n",
    "        for epoch in tepochs:\n",
    "            model.train()\n",
    "            # keep track of the running loss\n",
    "            running_loss = 0.\n",
    "            correct, total = 0, 0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            # getting the training set\n",
    "            data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        #squeeze out extra dimension in data\n",
    "        #data = data.squeeze(dim = 0) ########################################### this is new. \n",
    "\n",
    "        #using torch dataloader, you have to unsqueeze the data, I think...\n",
    "            data = data.reshape((100, 1, 128, 129))#######  this is hardcoded for now and should be fixed somewhere else. \n",
    "        #100 is the batch size, 1 is for a single channel, and 128 by 129 is the size of the spectrogram image in pixels. \n",
    "\n",
    "        # Get the model output (call the model with the data from this batch)\n",
    "            output = model(data)\n",
    "        # Zero the gradients out)\n",
    "            optimizer.zero_grad()\n",
    "        # Get the Loss\n",
    "            loss  = criterion(output, target)\n",
    "        # Calculate the gradients\n",
    "            loss.backward()\n",
    "        # Update the weights (using the training step of the optimizer)\n",
    "            optimizer.step()\n",
    "\n",
    "            tepochs.set_postfix(loss=loss.item())\n",
    "            running_loss += loss  # add the loss for this batch\n",
    "\n",
    "        # get accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "      # append the loss for this epoch\n",
    "            train_loss.append(running_loss/len(train_loader))\n",
    "            train_acc.append(correct/total)\n",
    "\n",
    "      # evaluate on validation data\n",
    "            model.eval()\n",
    "            running_loss = 0.\n",
    "            correct, total = 0, 0\n",
    "\n",
    "        for data, target in validation_loader:\n",
    "        # getting the validation set\n",
    "        #using torch dataloader, you have to unsqueeze the data, I think...\n",
    "            data = data.reshape((100, 1, 128, 129))#######  this is hardcoded for now and should be fixed somewhere else. \n",
    "        #100 is the batch size, 1 is for a single channel, and 128 by 129 is the size of the spectrogram image in pixels. \n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            tepochs.set_postfix(loss=loss.item())\n",
    "            running_loss += loss.item()\n",
    "        # get accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            validation_loss.append(running_loss/len(validation_loader))\n",
    "            validation_acc.append(correct/total)\n",
    "\n",
    "    return train_loss, train_acc, validation_loss, validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QxljFrGg6hEC"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746318eabea346f3a2587bb9cd50b01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-faf4f1e5bb41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run training.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmusic_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplot_loss_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-93c38e23f5dd>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, validation_loader, epochs)\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mcorrect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[1;31m# getting the training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchaudio\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, specgram, mask_value)\u001b[0m\n\u001b[0;32m   1136\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask_along_axis_iid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspecgram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1138\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspecgram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchaudio\\functional\\functional.py\u001b[0m in \u001b[0;36mmask_along_axis\u001b[1;34m(specgram, mask_param, mask_value, axis, p)\u001b[0m\n\u001b[0;32m    792\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"The value of p must be between 0.0 and 1.0 ({p} given).\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m     \u001b[0mmask_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_mask_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecgram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmask_param\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mspecgram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Run training.\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 30)\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwFcIYEy57_k"
   },
   "source": [
    "# Run training.\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 30)\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zeaz6LQg-Sde"
   },
   "source": [
    "Very interesting! We reach high training accuracy in much fewer epochs here than we did with the original png dataset. I should note that I am also using a larger batch size here than there. Could that be having an effect? \n",
    "\n",
    "We reach that high accuracy on the training set without any of the 'bumps' that we saw when training on the png dataset for longer. Maybe that's a sign that I should stop focusing so much on the RSA and instead focus on reducing the overfitting with this model. \n",
    "\n",
    "Some ideas on reducing the overfitting: \n",
    "- Data augmentation\n",
    "- More dropout\n",
    "- L1 / L2 reglularization (doesn't seem to be included at the moment). \n",
    "- Reduce the complexity of the model (fewer layers, narrower layers). \n",
    "- early stopping (although based on the plots, that doesn't seem like it would make a difference.) \n",
    "\n",
    "The validation accuracy is also very noisy. I already made the validation set larger than before (15% instead of 10%). Is there anything else we can do about that? Is it something we should be worried about? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6u-6c5nzhHJ"
   },
   "source": [
    "# Experimenting with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XviWncL0zkGM"
   },
   "source": [
    "Robert suggested that a good place to start would be simplifying the model. Strip it down to as few layers as possible until it can barely improve on the training set, then add complexity back in, looking for a sweetspot with good fit without massive overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9ozJ2dozsvI"
   },
   "source": [
    "## Reducing number of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR45bico1Vkb"
   },
   "source": [
    "# Alex's Advice on Regularization: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KITOaNu-bFEJ"
   },
   "source": [
    "Here are some ideas of changes to the model with Alex's help: \n",
    "\n",
    "- Increase the stride to decrease computation time (thanks Zach!)\n",
    "\n",
    "- Dropout after conv2D layer (thanks Courtnie!)\n",
    "\n",
    "- Create a bottleneck shape for the channels (wide to narrow to wide) (thanks Alex!)\n",
    "  - for example, instead of [1, 8, 32, 64, 128, 512] do [1, 64, 512, 128]. \n",
    "\n",
    "- Use only a subset of the training data for hyperparameter setting. That will make this iteration process faster. (thanks Alex!)\n",
    "\n",
    "- Try increasing stride instead of max pooling. (thanks Alex)\n",
    "\n",
    "- Play with learning rate a bit , but this is lower priority. (thanks Alex). \n",
    "  - Try a slightly lower learning rate. \n",
    "\n",
    "- More data augmentation should be priority #1. \n",
    "\n",
    "- With optimizers other than adam (simpler ones) learning rate decay usually improves accuracy. With adam it isn't so necessary. \n",
    "  - There are some cases where adam doesn't work so great. Hard to predict what those cases will be. Sometimes SGD + learning rate decay is better. It is usually about equally good to adam, but adam is simpler to set up because fewer parameters. \n",
    "\n",
    "-  Our next meeting with Alex will be on Thursday- same time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsHy0F4UiaMJ"
   },
   "source": [
    "Here are some ideas about augmentation in a dataloader with Alex: \n",
    "\n",
    "- Most direct option is to write a dataloader that applies the windowing, the augmentation, the spectrograming, all at the dataloader phase. \n",
    "\n",
    "- Another option would be to apply the transformation in the frequency domain, so that we apply the augmentations on the spectrograms, not the signal. \n",
    "  - this might be the better option, because it will be faster (don't need to fft every time). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQXQpW-lFQ7z"
   },
   "source": [
    "### Bottle Neck Shape of Network\n",
    "\n",
    "I'm going to use the same number of layers as the original network, but make the filter size get wide --> small --> big. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0xTFO3iFfLT"
   },
   "outputs": [],
   "source": [
    "# Make a CNN & train it to predict genres.\n",
    "\n",
    "class music_net(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"Intitalize neural net layers\"\"\"\n",
    "    super(music_net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=0) # changed in channels to 1 \n",
    "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv4 = nn.Conv2d(in_channels=128, out_channels=512, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv5 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "    self.fc1 = nn.Linear(in_features=512, out_features=10) ##################################### changed in_features\n",
    "\n",
    "    self.batchnorm1 = nn.BatchNorm2d(num_features=16)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(num_features=64)\n",
    "    self.batchnorm3 = nn.BatchNorm2d(num_features=128)\n",
    "    self.batchnorm4 = nn.BatchNorm2d(num_features=512)\n",
    "    self.batchnorm5 = nn.BatchNorm2d(num_features=128)\n",
    "\n",
    "    self.dropout = nn.Dropout(p=0.3, inplace=False)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Conv layer 1.\n",
    "    x = self.conv1(x)\n",
    "    x = self.batchnorm1(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 2.\n",
    "    x = self.conv2(x)\n",
    "    x = self.batchnorm2(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 3.\n",
    "    x = self.conv3(x)\n",
    "    x = self.batchnorm3(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 4.\n",
    "    x = self.conv4(x)\n",
    "    x = self.batchnorm4(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 5.\n",
    "    x = self.conv5(x)\n",
    "    x = self.batchnorm5(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    #print(x.shape)################################################################\n",
    "\n",
    "    # Fully connected layer 1.\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.dropout(x)\n",
    "    x = self.fc1(x)\n",
    "    x = F.softmax(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, validation_loader, epochs):\n",
    "  criterion =  nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "  train_loss, validation_loss = [], []\n",
    "  train_acc, validation_acc = [], []\n",
    "  with tqdm(range(epochs), unit='epoch') as tepochs:\n",
    "    tepochs.set_description('Training')\n",
    "    for epoch in tepochs:\n",
    "      model.train()\n",
    "      # keep track of the running loss\n",
    "      running_loss = 0.\n",
    "      correct, total = 0, 0\n",
    "\n",
    "      for data, target in train_loader:\n",
    "        # getting the training set\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        #squeeze out extra dimension in data\n",
    "        #data = data.squeeze(dim = 0) ########################################### this is new. \n",
    "\n",
    "        #using torch dataloader, you have to unsqueeze the data, I think...\n",
    "        data = data.reshape((100, 1, 128, 129))#######  this is hardcoded for now and should be fixed somewhere else. \n",
    "        #100 is the batch size, 1 is for a single channel, and 128 by 129 is the size of the spectrogram image in pixels. \n",
    "\n",
    "        # Get the model output (call the model with the data from this batch)\n",
    "        output = model(data)\n",
    "        # Zero the gradients out)\n",
    "        optimizer.zero_grad()\n",
    "        # Get the Loss\n",
    "        loss  = criterion(output, target)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights (using the training step of the optimizer)\n",
    "        optimizer.step()\n",
    "\n",
    "        tepochs.set_postfix(loss=loss.item())\n",
    "        running_loss += loss  # add the loss for this batch\n",
    "\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "      # append the loss for this epoch\n",
    "      train_loss.append(running_loss/len(train_loader))\n",
    "      train_acc.append(correct/total)\n",
    "\n",
    "      # evaluate on validation data\n",
    "      model.eval()\n",
    "      running_loss = 0.\n",
    "      correct, total = 0, 0\n",
    "\n",
    "      for data, target in validation_loader:\n",
    "        # getting the validation set\n",
    "        #using torch dataloader, you have to unsqueeze the data, I think...\n",
    "        data = data.reshape((100, 1, 128, 129))#######  this is hardcoded for now and should be fixed somewhere else. \n",
    "        #100 is the batch size, 1 is for a single channel, and 128 by 129 is the size of the spectrogram image in pixels. \n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        tepochs.set_postfix(loss=loss.item())\n",
    "        running_loss += loss.item()\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "      validation_loss.append(running_loss/len(validation_loader))\n",
    "      validation_acc.append(correct/total)\n",
    "\n",
    "  return train_loss, train_acc, validation_loss, validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475,
     "referenced_widgets": [
      "2f5a538b5bad4070a29f49079ea60615",
      "eeb5b2ca066b4a198ebf3e9a7da59cf4",
      "75120dc7a96a4420bd1d4f81cafbb677",
      "44c07e071e5a4e41a2364597439b00cb",
      "424b11fd66e74ec584db03c70fe15a7b",
      "7857881ed4784d86ab5447e4d123cc33",
      "b83c8f4d01a4419d8e27e5a308b39d1b",
      "932c752fd5fb42f4a714f8fc372d08cb"
     ]
    },
    "id": "OLxUNlToGPnb",
    "outputId": "4e04b4a5-caa3-4279-d6c5-2fd3892dd2ea"
   },
   "outputs": [],
   "source": [
    "# Run training.\n",
    "\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 50)\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sBaBe_yGSaU"
   },
   "source": [
    "That did help it learn the training set faster, compared to a network with the same number of layers but the size was doubling at every layer, but it didn't change much with the validation set accuracy. Maybe I'll stick with this architecture and try applying other regularization tricks to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-aa2-ZoJ63p"
   },
   "source": [
    "### Data Augmentation with SpecAugment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zncnEZv3J97L"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFWKKNkFKHHI"
   },
   "outputs": [],
   "source": [
    "# Data loading.\n",
    "train_dir = \"/content/train_cropped/\"\n",
    "val_dir =  \"/content/val_cropped/\"\n",
    "\n",
    "train_dataset = datasets.DatasetFolder(\n",
    "    train_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load, \n",
    "    transform = transforms.Compose([torchaudio.transforms.TimeMasking(time_mask_param= 10, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param=5, iid_masks=True)]) )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)\n",
    "\n",
    "val_dataset = datasets.DatasetFolder(\n",
    "    val_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lmps3SHZKrS6"
   },
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpfJ8cypLj1A"
   },
   "outputs": [],
   "source": [
    "for data, target in train_dataset:\n",
    "  print(target)\n",
    "  print(data.shape)\n",
    "  plt.figure()\n",
    "  plt.imshow(data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441,
     "referenced_widgets": [
      "e552dfa912344996a8835f468d1f27d3",
      "24c382687bca41c4bab00171a7d36831",
      "7183265a3d0f4baf95a4007076a41980",
      "bf6047816f904ba681f338e2bb6d655a",
      "8ee7e3462caa4ad5bab4253d08daa171",
      "7390849f069e4bc892baa0c7e25c6188",
      "048df7632d1642c6b2570fe1840069eb",
      "a71598188af648b8beb7d763ea05c710"
     ]
    },
    "id": "eDKs7C1zMgXI",
    "outputId": "dcb3dc1d-617b-4a42-8d2c-f92741b46057"
   },
   "outputs": [],
   "source": [
    "# Run training.\n",
    "\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 50)\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsMtKxDEM3nS"
   },
   "source": [
    "It seems that that level of masking really didn't have much effect at all. Maybe we need wider masking bands? I don't know how much we can push it really. I'll try again with much larger masks and fewer epochs just to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttTdI5DWRPzi"
   },
   "outputs": [],
   "source": [
    "# Data loading.\n",
    "train_dir = \"/content/train_cropped/\"\n",
    "val_dir =  \"/content/val_cropped/\"\n",
    "\n",
    "train_dataset = datasets.DatasetFolder(\n",
    "    train_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load, \n",
    "    transform = transforms.Compose([torchaudio.transforms.TimeMasking(time_mask_param= 20, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param=20, iid_masks=True), \n",
    "                                    torchaudio.transforms.TimeMasking(time_mask_param= 20, iid_masks=True)]) )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)\n",
    "\n",
    "val_dataset = datasets.DatasetFolder(\n",
    "    val_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475,
     "referenced_widgets": [
      "903d46ce405e49688fe86e8091c3075d",
      "14a1d55b468842de91527b6ea7474212",
      "ad86d462f45f4f899f79b3526039732d",
      "a54a239906e54b55a7febfe3c67b3dd8",
      "c5be2588bf1e4551a65f601812ba59b9",
      "b109d0bb482846889d3f2b10ca6b9dc4",
      "ae6aa230b12e48b48dfadd974dd84e25",
      "a67e6142bb5a481eb6f7fb3e7e92e68d"
     ]
    },
    "id": "WO7knWhkRbKd",
    "outputId": "932ff924-8093-4a40-cccb-09bc6ef58a9e"
   },
   "outputs": [],
   "source": [
    "# Run training.\n",
    "\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 50)\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJcqUjQOReYb"
   },
   "source": [
    "This does seem to have helped. The validation accuracy is now hovering around the mid 70s, not the low 70s it seems. Maybe we can push the masking even further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBBKTbT4SnwL"
   },
   "outputs": [],
   "source": [
    "# Data loading.\n",
    "train_dir = \"/content/train_cropped/\"\n",
    "val_dir =  \"/content/val_cropped/\"\n",
    "\n",
    "train_dataset = datasets.DatasetFolder(\n",
    "    train_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load, \n",
    "    transform = transforms.Compose([torchaudio.transforms.TimeMasking(time_mask_param= 40, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param=40, iid_masks=True), \n",
    "                                    torchaudio.transforms.TimeMasking(time_mask_param= 20, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param = 20, iid_masks=True)]) )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)\n",
    "\n",
    "val_dataset = datasets.DatasetFolder(\n",
    "    val_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499,
     "referenced_widgets": [
      "6905fce7caed4c8eb6b9132456b7ce83",
      "3fc6010ddb0a4343957259a35d409bde",
      "4dbcccdd47494b0080a3ddb58f78db79",
      "add790de876f40a4a776b6bf510ec7ae",
      "70d90a18ab5b4325ad6d775448fdc9f5",
      "cbe0162577a5424b90d2a0d1dc9c4dee",
      "f588fbb7090d4c7b82eee080c1c6280c",
      "a94ecc5031594ce3b165c1221373a361"
     ]
    },
    "id": "tzaO-JJNTTVO",
    "outputId": "3c76bf64-266f-4643-f0c8-09ba5dbccfa3"
   },
   "outputs": [],
   "source": [
    "# Run training.\n",
    "\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 50)\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1eown8uTVZG"
   },
   "source": [
    "Looks like that is going even better! I'm not to increase the augmentation ***even more*** and also increase the learning rate slightly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-2_iyTOpbHKS"
   },
   "outputs": [],
   "source": [
    "# Data loading.\n",
    "train_dir = \"/content/train_cropped/\"\n",
    "val_dir =  \"/content/val_cropped/\"\n",
    "\n",
    "train_dataset = datasets.DatasetFolder(\n",
    "    train_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load, \n",
    "    transform = transforms.Compose([torchaudio.transforms.TimeMasking(time_mask_param= 40, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param=40, iid_masks=True), \n",
    "                                    torchaudio.transforms.TimeMasking(time_mask_param= 30, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param = 30, iid_masks=True), \n",
    "                                    torchaudio.transforms.TimeMasking(time_mask_param= 20, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param = 20, iid_masks=True)]) )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)\n",
    "\n",
    "val_dataset = datasets.DatasetFolder(\n",
    "    val_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05hoh57SbRGi"
   },
   "outputs": [],
   "source": [
    "# Make a CNN & train it to predict genres.\n",
    "\n",
    "class music_net(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"Intitalize neural net layers\"\"\"\n",
    "    super(music_net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=0) # changed in channels to 1 \n",
    "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv4 = nn.Conv2d(in_channels=128, out_channels=512, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv5 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "    self.fc1 = nn.Linear(in_features=512, out_features=10) ##################################### changed in_features\n",
    "\n",
    "    self.batchnorm1 = nn.BatchNorm2d(num_features=16)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(num_features=64)\n",
    "    self.batchnorm3 = nn.BatchNorm2d(num_features=128)\n",
    "    self.batchnorm4 = nn.BatchNorm2d(num_features=512)\n",
    "    self.batchnorm5 = nn.BatchNorm2d(num_features=128)\n",
    "\n",
    "    self.dropout = nn.Dropout(p=0.3, inplace=False)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Conv layer 1.\n",
    "    x = self.conv1(x)\n",
    "    x = self.batchnorm1(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 2.\n",
    "    x = self.conv2(x)\n",
    "    x = self.batchnorm2(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 3.\n",
    "    x = self.conv3(x)\n",
    "    x = self.batchnorm3(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 4.\n",
    "    x = self.conv4(x)\n",
    "    x = self.batchnorm4(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 5.\n",
    "    x = self.conv5(x)\n",
    "    x = self.batchnorm5(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    #print(x.shape)################################################################\n",
    "\n",
    "    # Fully connected layer 1.\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.dropout(x)\n",
    "    x = self.fc1(x)\n",
    "    x = F.softmax(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, validation_loader, epochs):\n",
    "  criterion =  nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "  train_loss, validation_loss = [], []\n",
    "  train_acc, validation_acc = [], []\n",
    "  with tqdm(range(epochs), unit='epoch') as tepochs:\n",
    "    tepochs.set_description('Training')\n",
    "    for epoch in tepochs:\n",
    "      model.train()\n",
    "      # keep track of the running loss\n",
    "      running_loss = 0.\n",
    "      correct, total = 0, 0\n",
    "\n",
    "      for data, target in train_loader:\n",
    "        # getting the training set\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        #squeeze out extra dimension in data\n",
    "        #data = data.squeeze(dim = 0) ########################################### this is new. \n",
    "\n",
    "        #using torch dataloader, you have to unsqueeze the data, I think...\n",
    "        data = data.reshape((100, 1, 128, 129))#######  this is hardcoded for now and should be fixed somewhere else. \n",
    "        #100 is the batch size, 1 is for a single channel, and 128 by 129 is the size of the spectrogram image in pixels. \n",
    "\n",
    "        # Get the model output (call the model with the data from this batch)\n",
    "        output = model(data)\n",
    "        # Zero the gradients out)\n",
    "        optimizer.zero_grad()\n",
    "        # Get the Loss\n",
    "        loss  = criterion(output, target)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights (using the training step of the optimizer)\n",
    "        optimizer.step()\n",
    "\n",
    "        tepochs.set_postfix(loss=loss.item())\n",
    "        running_loss += loss  # add the loss for this batch\n",
    "\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "      # append the loss for this epoch\n",
    "      train_loss.append(running_loss/len(train_loader))\n",
    "      train_acc.append(correct/total)\n",
    "\n",
    "      # evaluate on validation data\n",
    "      model.eval()\n",
    "      running_loss = 0.\n",
    "      correct, total = 0, 0\n",
    "\n",
    "      for data, target in validation_loader:\n",
    "        # getting the validation set\n",
    "        #using torch dataloader, you have to unsqueeze the data, I think...\n",
    "        data = data.reshape((100, 1, 128, 129))#######  this is hardcoded for now and should be fixed somewhere else. \n",
    "        #100 is the batch size, 1 is for a single channel, and 128 by 129 is the size of the spectrogram image in pixels. \n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        tepochs.set_postfix(loss=loss.item())\n",
    "        running_loss += loss.item()\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "      validation_loss.append(running_loss/len(validation_loader))\n",
    "      validation_acc.append(correct/total)\n",
    "\n",
    "  return train_loss, train_acc, validation_loss, validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "4c9d01d192c34f67922e4566d0474d74",
      "693a904bc74c4661a26bb4352e5a3335",
      "93745946cc8d40d09992ca9e8a205243",
      "e686e8dd897f4509961cb99e2cbc9f0c",
      "1724869d5fa04d208b4b574311e95c8c",
      "d420664ff4604f3fb976b86a4e6e59c4",
      "73e6dcbae47d4cda9bcf8503920f7305",
      "a0228209059c49f8b19bdf17ed2c0f87"
     ]
    },
    "id": "bMran6yJbdoD",
    "outputId": "6a69c43e-1072-41b3-fc03-fc9d507bb642"
   },
   "outputs": [],
   "source": [
    "# Run training.\n",
    "\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 50)\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkRF0bVgbhmA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "210816_ZI__music_classify_",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "048df7632d1642c6b2570fe1840069eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14a1d55b468842de91527b6ea7474212": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1724869d5fa04d208b4b574311e95c8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "24c382687bca41c4bab00171a7d36831": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f5a538b5bad4070a29f49079ea60615": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_75120dc7a96a4420bd1d4f81cafbb677",
       "IPY_MODEL_44c07e071e5a4e41a2364597439b00cb"
      ],
      "layout": "IPY_MODEL_eeb5b2ca066b4a198ebf3e9a7da59cf4"
     }
    },
    "3fc6010ddb0a4343957259a35d409bde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "424b11fd66e74ec584db03c70fe15a7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "44c07e071e5a4e41a2364597439b00cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_932c752fd5fb42f4a714f8fc372d08cb",
      "placeholder": "",
      "style": "IPY_MODEL_b83c8f4d01a4419d8e27e5a308b39d1b",
      "value": " 50/50 [24:42&lt;00:00, 29.65s/epoch, loss=1.74]"
     }
    },
    "4c9d01d192c34f67922e4566d0474d74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_93745946cc8d40d09992ca9e8a205243",
       "IPY_MODEL_e686e8dd897f4509961cb99e2cbc9f0c"
      ],
      "layout": "IPY_MODEL_693a904bc74c4661a26bb4352e5a3335"
     }
    },
    "4dbcccdd47494b0080a3ddb58f78db79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Training: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbe0162577a5424b90d2a0d1dc9c4dee",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_70d90a18ab5b4325ad6d775448fdc9f5",
      "value": 50
     }
    },
    "6905fce7caed4c8eb6b9132456b7ce83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4dbcccdd47494b0080a3ddb58f78db79",
       "IPY_MODEL_add790de876f40a4a776b6bf510ec7ae"
      ],
      "layout": "IPY_MODEL_3fc6010ddb0a4343957259a35d409bde"
     }
    },
    "693a904bc74c4661a26bb4352e5a3335": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70d90a18ab5b4325ad6d775448fdc9f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7183265a3d0f4baf95a4007076a41980": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Training: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7390849f069e4bc892baa0c7e25c6188",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8ee7e3462caa4ad5bab4253d08daa171",
      "value": 50
     }
    },
    "7390849f069e4bc892baa0c7e25c6188": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73e6dcbae47d4cda9bcf8503920f7305": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75120dc7a96a4420bd1d4f81cafbb677": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Training: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7857881ed4784d86ab5447e4d123cc33",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_424b11fd66e74ec584db03c70fe15a7b",
      "value": 50
     }
    },
    "7857881ed4784d86ab5447e4d123cc33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ee7e3462caa4ad5bab4253d08daa171": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "903d46ce405e49688fe86e8091c3075d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ad86d462f45f4f899f79b3526039732d",
       "IPY_MODEL_a54a239906e54b55a7febfe3c67b3dd8"
      ],
      "layout": "IPY_MODEL_14a1d55b468842de91527b6ea7474212"
     }
    },
    "932c752fd5fb42f4a714f8fc372d08cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93745946cc8d40d09992ca9e8a205243": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "Training:   0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d420664ff4604f3fb976b86a4e6e59c4",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1724869d5fa04d208b4b574311e95c8c",
      "value": 0
     }
    },
    "a0228209059c49f8b19bdf17ed2c0f87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a54a239906e54b55a7febfe3c67b3dd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a67e6142bb5a481eb6f7fb3e7e92e68d",
      "placeholder": "",
      "style": "IPY_MODEL_ae6aa230b12e48b48dfadd974dd84e25",
      "value": " 50/50 [19:03&lt;00:00, 22.87s/epoch, loss=1.74]"
     }
    },
    "a67e6142bb5a481eb6f7fb3e7e92e68d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a71598188af648b8beb7d763ea05c710": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a94ecc5031594ce3b165c1221373a361": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad86d462f45f4f899f79b3526039732d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Training: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b109d0bb482846889d3f2b10ca6b9dc4",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c5be2588bf1e4551a65f601812ba59b9",
      "value": 50
     }
    },
    "add790de876f40a4a776b6bf510ec7ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a94ecc5031594ce3b165c1221373a361",
      "placeholder": "",
      "style": "IPY_MODEL_f588fbb7090d4c7b82eee080c1c6280c",
      "value": " 50/50 [27:40&lt;00:00, 33.20s/epoch, loss=1.77]"
     }
    },
    "ae6aa230b12e48b48dfadd974dd84e25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b109d0bb482846889d3f2b10ca6b9dc4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b83c8f4d01a4419d8e27e5a308b39d1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf6047816f904ba681f338e2bb6d655a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a71598188af648b8beb7d763ea05c710",
      "placeholder": "",
      "style": "IPY_MODEL_048df7632d1642c6b2570fe1840069eb",
      "value": " 50/50 [23:46&lt;00:00, 28.53s/epoch, loss=1.75]"
     }
    },
    "c5be2588bf1e4551a65f601812ba59b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cbe0162577a5424b90d2a0d1dc9c4dee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d420664ff4604f3fb976b86a4e6e59c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e552dfa912344996a8835f468d1f27d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7183265a3d0f4baf95a4007076a41980",
       "IPY_MODEL_bf6047816f904ba681f338e2bb6d655a"
      ],
      "layout": "IPY_MODEL_24c382687bca41c4bab00171a7d36831"
     }
    },
    "e686e8dd897f4509961cb99e2cbc9f0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0228209059c49f8b19bdf17ed2c0f87",
      "placeholder": "",
      "style": "IPY_MODEL_73e6dcbae47d4cda9bcf8503920f7305",
      "value": " 0/50 [00:27&lt;?, ?epoch/s, loss=2.01]"
     }
    },
    "eeb5b2ca066b4a198ebf3e9a7da59cf4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f588fbb7090d4c7b82eee080c1c6280c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
