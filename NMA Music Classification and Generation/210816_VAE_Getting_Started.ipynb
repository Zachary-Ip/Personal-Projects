{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " 210816 - VAE Getting Started.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eEWYENkfiDYY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4ae8b5ee49384298994f5380b3722d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7a839406b343411a9b70bfefee27502c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dc9267322fa242608874d993f763f5af",
              "IPY_MODEL_2fd4ec1c5d704e128410bb2563a550df",
              "IPY_MODEL_5a42a3b718c847878fd158c3769e4271"
            ]
          }
        },
        "7a839406b343411a9b70bfefee27502c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc9267322fa242608874d993f763f5af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0c05781fdeed453a98fcb8bc08d0bc6b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epochs:   0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9e70d2d4416c45fd8753f1814c166819"
          }
        },
        "2fd4ec1c5d704e128410bb2563a550df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_89700f2db11344919258468e455c61e5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_46028d986f8145499ebf6db8b8d74210"
          }
        },
        "5a42a3b718c847878fd158c3769e4271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7f646955518e43309ea3c6c60dbbb790",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/10 [00:07&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6282fea65ddd4660a2712e325e2c636f"
          }
        },
        "0c05781fdeed453a98fcb8bc08d0bc6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9e70d2d4416c45fd8753f1814c166819": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89700f2db11344919258468e455c61e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "46028d986f8145499ebf6db8b8d74210": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f646955518e43309ea3c6c60dbbb790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6282fea65ddd4660a2712e325e2c636f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "90696c74e3ba42338356293ff732b0d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_25471ab6a3b24847ab7849d7a009caf9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f907dd5cc3d84825bdb9ee3b8f1e7c7d",
              "IPY_MODEL_ad955c114d184512a38c67b6e5a34aaa",
              "IPY_MODEL_e59a0579cd4347878ae6016cff095ffc"
            ]
          }
        },
        "25471ab6a3b24847ab7849d7a009caf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f907dd5cc3d84825bdb9ee3b8f1e7c7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d7e5ae4a45db4b0aa3eca9c64393c8a5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Batches:  12%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a107a89d74b14933b6dd47071788e53e"
          }
        },
        "ad955c114d184512a38c67b6e5a34aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9b25a4daa3ae4d62b562c5243a3383cf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 78,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8457afbfa0434866a9a21d0cc723dec2"
          }
        },
        "e59a0579cd4347878ae6016cff095ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4754c0b6213c44489e36606b3b0079ad",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9/78 [00:07&lt;00:53,  1.30it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e2921381767d436e8b408f57a2a5984b"
          }
        },
        "d7e5ae4a45db4b0aa3eca9c64393c8a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a107a89d74b14933b6dd47071788e53e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b25a4daa3ae4d62b562c5243a3383cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8457afbfa0434866a9a21d0cc723dec2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4754c0b6213c44489e36606b3b0079ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e2921381767d436e8b408f57a2a5984b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTejwTFHhukQ"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import imageio\n",
        "import random, shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as display\n",
        "import librosa\n",
        "import librosa.display\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "#import torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wyOcTZRkS9L",
        "outputId": "7cf077d8-ae59-4c86-8a90-6cb2c4ed5170"
      },
      "source": [
        "def set_seed(seed=None, seed_torch=True):\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "set_seed(2021)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random seed 2021 has been set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VALFyiy9pmCO",
        "outputId": "78da37a6-bd8c-4bb9-c8ad-599e51312215"
      },
      "source": [
        "def set_device():\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"WARNING: For this notebook to perform best, \"\n",
        "        \"if possible, in the menu under `Runtime` -> \"\n",
        "        \"`Change runtime type.`  select `GPU` \")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "  return device\n",
        "\n",
        "DEVICE = set_device()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is enabled in this notebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUu7Uv4Yg9P7"
      },
      "source": [
        "From W2D5 Tutorial: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEWYENkfiDYY"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCz5m26ciGOo"
      },
      "source": [
        "import requests\n",
        "\n",
        "fname = \"music.zip\"\n",
        "url = \"https://osf.io/drjhb/download\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KpQU-EtiWgD"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile(fname, 'r') as zipObj:\n",
        "  # Extract all the contents of zip file in different directory\n",
        "  zipObj.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPS6b_CKiYTH",
        "outputId": "16f22450-4b9c-4829-9306-5c57648a81ca"
      },
      "source": [
        "in_folder_path = \"/content/Data/genres_original/\"\n",
        "out_folder_path = \"/content/spectrograms_30s/\" #change this for your personal drive. \n",
        "\n",
        "if not os.path.exists(out_folder_path):\n",
        "    os.mkdir(out_folder_path)\n",
        "\n",
        "#get list of genre folders\n",
        "genre_folders = glob.glob(in_folder_path + \"*\")\n",
        "\n",
        "#initialize empty list of problem files:\n",
        "problem_files = []\n",
        "\n",
        "#loop over genre folders\n",
        "for genre_folder in genre_folders: \n",
        "  #get list of individual .wav files\n",
        "  wav_files = glob.glob(genre_folder + \"/*.wav\")\n",
        "\n",
        "  #get output folder path\n",
        "  genre_name = genre_folder.split('/')[-1]\n",
        "  print(genre_name)\n",
        "  out_folder = out_folder_path + genre_name + \"/\"\n",
        "  #make output folder if it doesn't exist\n",
        "  if not os.path.exists(out_folder):\n",
        "    os.mkdir(out_folder)\n",
        "\n",
        "  #loop over list of wav files\n",
        "  for wav_file in wav_files:\n",
        "    #extract file name\n",
        "    file_name = wav_file.split('/')[-1]\n",
        "    file_name = file_name.strip(\".wav\")\n",
        "    \n",
        "    #load wav file\n",
        "    try: \n",
        "      y, sr = librosa.load(wav_file)\n",
        "\n",
        "      #normalize audio\n",
        "      y = (y - y.mean())/ y.std()\n",
        "\n",
        "      #calculate mel spectrogram\n",
        "      spectrogram = librosa.feature.melspectrogram(y, sr = sr)\n",
        "      #spectrogram_db = librosa.amplitude_to_db(spectrogram, ref = np.max)\n",
        "\n",
        "      #global normalization of the spectrogram (not frequency band normalization...)\n",
        "      #spectrogram_norm = (spectrogram_db - spectrogram_db.mean()) / spectrogram_db.std()\n",
        "\n",
        "      #save spectrogram\n",
        "      np.save(out_folder + file_name, spectrogram)\n",
        "\n",
        "    except:\n",
        "      print(\"Something is wrong with \" + file_name + \"so it has been skipped\")\n",
        "      problem_files = problem_files.append(wav_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reggae\n",
            "disco\n",
            "country\n",
            "pop\n",
            "blues\n",
            "rock\n",
            "hiphop\n",
            "metal\n",
            "jazz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Something is wrong with jazz.00054so it has been skipped\n",
            "classical\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCL415C_icmt"
      },
      "source": [
        "## EDIT HERE TO CHANGE SIZE OF TEST AND VAL DATASETS\n",
        "test_prop = 0.15\n",
        "val_prop = 0.15\n",
        "\n",
        "train_prop = 1 - test_prop - val_prop\n",
        "\n",
        "# Create folder with training, testing and validation data.\n",
        "\n",
        "spectrograms_dir = \"/content/spectrograms_30s/\"\n",
        "folder_names = ['/content/train/', \n",
        "                '/content/test/', \n",
        "                '/content/val/']\n",
        "train_dir = folder_names[0]\n",
        "test_dir = folder_names[1]\n",
        "val_dir = folder_names[2]\n",
        "\n",
        "for f in folder_names:\n",
        "  if os.path.exists(f):\n",
        "    shutil.rmtree(f)\n",
        "    os.mkdir(f)\n",
        "  else:\n",
        "    os.mkdir(f)\n",
        "\n",
        "# Loop over all genres.\n",
        "\n",
        "genres = list(os.listdir(spectrograms_dir))\n",
        "for g in genres:\n",
        "  # find all images & split in train, test, and validation\n",
        "  src_file_paths= []\n",
        "  for im in glob.glob(os.path.join(spectrograms_dir, f'{g}',\"*.npy\"), recursive=True):\n",
        "    src_file_paths.append(im)\n",
        "  random.Random(2021).shuffle(src_file_paths)\n",
        "\n",
        "  test_idx = int(len(src_file_paths) * test_prop)\n",
        "  val_idx = test_idx + int(len(src_file_paths) * val_prop)\n",
        "\n",
        "  test_files = src_file_paths[0:test_idx]\n",
        "  val_files = src_file_paths[test_idx:val_idx]\n",
        "  train_files = src_file_paths[val_idx:]\n",
        "\n",
        "  #  make destination folders for train and test images\n",
        "  for f in folder_names:\n",
        "    if not os.path.exists(os.path.join(f + f\"{g}\")):\n",
        "      os.mkdir(os.path.join(f + f\"{g}\"))\n",
        "\n",
        "  # copy training and testing images over\n",
        "  for f in train_files:\n",
        "    shutil.copy(f, os.path.join(os.path.join(train_dir + f\"{g}\") + '/',os.path.split(f)[1]))\n",
        "  for f in test_files:\n",
        "    shutil.copy(f, os.path.join(os.path.join(test_dir + f\"{g}\") + '/',os.path.split(f)[1]))\n",
        "  for f in val_files:\n",
        "    shutil.copy(f, os.path.join(os.path.join(val_dir + f\"{g}\") + '/',os.path.split(f)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHVAAnxljYOG"
      },
      "source": [
        "def apply_sliding_window(in_dir, out_dir, window_length = 3, hop_length = 1, \n",
        "                         save_as_tensor = False):\n",
        "  #IMPORTANT NOTE - this function assumes that the spectrograms were made with \n",
        "    #default librosa nfft_size, hop_length, etc. \n",
        "\n",
        "    #window and hop length units are in seconds. \n",
        "\n",
        "  window_size = librosa.time_to_frames(window_length)\n",
        "  hop_size = librosa.time_to_frames(hop_length)\n",
        "  #get list of genre folders\n",
        "  genre_folders = glob.glob(in_dir + \"*\")\n",
        "\n",
        "  #make out_dir if it doesn't exist\n",
        "  if not os.path.exists(out_dir):\n",
        "    os.mkdir(out_dir)\n",
        "\n",
        "  #loop over genre folders\n",
        "  for genre_folder in genre_folders:\n",
        "\n",
        "    #get list of individual spectrogram files\n",
        "    spec_files = glob.glob(genre_folder + \"/*.npy\")\n",
        "\n",
        "    #get output folder path\n",
        "    genre_name = genre_folder.split('/')[-1]\n",
        "    out_folder = out_dir + genre_name + \"/\"\n",
        "    print(out_folder)\n",
        "    #make output folder if it doesn't exist\n",
        "\n",
        "    if not os.path.exists(out_folder):\n",
        "      os.mkdir(out_folder)\n",
        "\n",
        "    #loop over spectrogram files\n",
        "    for spec_file in spec_files:\n",
        "      #extract file name\n",
        "      file_name = spec_file.split('/')[-1]\n",
        "      file_name = file_name.strip(\".npy\")\n",
        "\n",
        "      #load spectrogram\n",
        "      spec = np.load(spec_file)\n",
        "      \n",
        "      #apply sliding frame to spectrogram\n",
        "      all_frames = librosa.util.frame(spec, window_size, hop_size)\n",
        "      all_frames = np.moveaxis(all_frames, 2, 0)\n",
        "\n",
        "      #loop over individual frames\n",
        "      for i, frame in enumerate(all_frames):\n",
        "        \n",
        "        #specify out file name\n",
        "        full_out_path = out_folder + file_name + \"_\" + str(i)\n",
        "        \n",
        "        if save_as_tensor: \n",
        "          #convert np array to tensor\n",
        "          torch_frame = torch.from_numpy(frame)\n",
        "          #save torch of spectrogram frame\n",
        "          torch.save(torch_frame, full_out_path + \".pt\")  \n",
        "\n",
        "        else: \n",
        "          #save spectrogram frame\n",
        "          np.save(full_out_path, frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O73sh4pfjete",
        "outputId": "39225244-9e52-4a8b-c332-67446ec8b0b5"
      },
      "source": [
        "# Cut Data \n",
        "full_path = '/content/'\n",
        "\n",
        "apply_sliding_window(full_path + 'test/*', full_path + 'test_cropped/', save_as_tensor=True)\n",
        "apply_sliding_window(full_path + 'train/*', full_path + 'train_cropped/', save_as_tensor=True)\n",
        "apply_sliding_window(full_path + 'val/*', full_path + 'val_cropped/', save_as_tensor = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/test_cropped/reggae/\n",
            "/content/test_cropped/disco/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/util/utils.py:200: UserWarning: librosa.util.frame called with axis=-1 on a non-contiguous input. This will result in a copy.\n",
            "  \"on a non-contiguous input. This will result in a copy.\".format(axis)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/test_cropped/country/\n",
            "/content/test_cropped/pop/\n",
            "/content/test_cropped/blues/\n",
            "/content/test_cropped/rock/\n",
            "/content/test_cropped/hiphop/\n",
            "/content/test_cropped/metal/\n",
            "/content/test_cropped/jazz/\n",
            "/content/test_cropped/classical/\n",
            "/content/train_cropped/reggae/\n",
            "/content/train_cropped/disco/\n",
            "/content/train_cropped/country/\n",
            "/content/train_cropped/pop/\n",
            "/content/train_cropped/blues/\n",
            "/content/train_cropped/rock/\n",
            "/content/train_cropped/hiphop/\n",
            "/content/train_cropped/metal/\n",
            "/content/train_cropped/jazz/\n",
            "/content/train_cropped/classical/\n",
            "/content/val_cropped/reggae/\n",
            "/content/val_cropped/disco/\n",
            "/content/val_cropped/country/\n",
            "/content/val_cropped/pop/\n",
            "/content/val_cropped/blues/\n",
            "/content/val_cropped/rock/\n",
            "/content/val_cropped/hiphop/\n",
            "/content/val_cropped/metal/\n",
            "/content/val_cropped/jazz/\n",
            "/content/val_cropped/classical/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVU9stD9jhsi"
      },
      "source": [
        "## Train the Conv Auto Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40jes_okoTAw"
      },
      "source": [
        "class BiasLayer(nn.Module):\n",
        "  def __init__(self, shape):\n",
        "    super(BiasLayer, self).__init__()\n",
        "    init_bias = torch.zeros(shape)\n",
        "    self.bias = nn.Parameter(init_bias, requires_grad=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdFkszyAofy8"
      },
      "source": [
        "def cout(x, layer):\n",
        "  \"\"\"Unnecessarily complicated but complete way to\n",
        "  calculate the output depth, height and width size for a Conv2D layer\n",
        "\n",
        "  Args:\n",
        "    x (tuple): input size (depth, height, width)\n",
        "    layer (nn.Conv2d): the Conv2D layer\n",
        "\n",
        "  returns:\n",
        "    (int): output shape as given in [Ref]\n",
        "\n",
        "  Ref:\n",
        "    https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "  \"\"\"\n",
        "  assert isinstance(layer, nn.Conv2d)\n",
        "  p = layer.padding if isinstance(layer.padding, tuple) else (layer.padding,)\n",
        "  k = layer.kernel_size if isinstance(layer.kernel_size, tuple) else (layer.kernel_size,)\n",
        "  d = layer.dilation if isinstance(layer.dilation, tuple) else (layer.dilation,)\n",
        "  s = layer.stride if isinstance(layer.stride, tuple) else (layer.stride,)\n",
        "  in_depth, in_height, in_width = x\n",
        "  out_depth = layer.out_channels\n",
        "  out_height = 1 + (in_height + 2 * p[0] - (k[0] - 1) * d[0] - 1) // s[0]\n",
        "  out_width = 1 + (in_width + 2 * p[-1] - (k[-1] - 1) * d[-1] - 1) // s[-1]\n",
        "  return (out_depth, out_height, out_width)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrA6BU7mqBno"
      },
      "source": [
        "## ConvVAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKcanA_msbpM"
      },
      "source": [
        "class BiasLayer(nn.Module):\n",
        "  def __init__(self, shape):\n",
        "    super(BiasLayer, self).__init__()\n",
        "    init_bias = torch.zeros(shape)\n",
        "    self.bias = nn.Parameter(init_bias, requires_grad=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    print(x.shape)\n",
        "    return x + self.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_qzsFbvuDvn"
      },
      "source": [
        "def log_p_x(x, mu_xs, sig_x):\n",
        "  \"\"\"Given [batch, ...] input x and [batch, n, ...] reconstructions, compute\n",
        "  pixel-wise log Gaussian probability\n",
        "\n",
        "  Sum over pixel dimensions, but mean over batch and samples.\n",
        "  \"\"\"\n",
        "  b, n = mu_xs.size()[:2]\n",
        "  # Flatten out pixels and add a singleton dimension [1] so that x will be\n",
        "  # implicitly expanded when combined with mu_xs\n",
        "  x = x.reshape(b, 1, -1)\n",
        "  _, _, p = x.size()\n",
        "  squared_error = (x - mu_xs.view(b, n, -1))**2 / (2*sig_x**2)\n",
        "\n",
        "  # Size of squared_error is [b,n,p]. log prob is by definition sum over [p].\n",
        "  # Expected value requires mean over [n]. Handling different size batches\n",
        "  # requires mean over [b].\n",
        "  return -(squared_error + torch.log(sig_x)).sum(dim=2).mean(dim=(0,1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwB1t70ruLd5"
      },
      "source": [
        "def kl_q_p(zs, phi):\n",
        "  \"\"\"Given [b,n,k] samples of z drawn from q, compute estimate of KL(q||p).\n",
        "  phi must be size [b,k+1]\n",
        "\n",
        "  This uses mu_p = 0 and sigma_p = 1, which simplifies the log(p(zs)) term to\n",
        "  just -1/2*(zs**2)\n",
        "  \"\"\"\n",
        "  b, n, k = zs.size()\n",
        "  mu_q, log_sig_q = phi[:,:-1], phi[:,-1]\n",
        "  log_p = -0.5*(zs**2)\n",
        "  log_q = -0.5*(zs - mu_q.view(b,1,k))**2 / log_sig_q.exp().view(b,1,1)**2 - log_sig_q.view(b,1,-1)\n",
        "  # Size of log_q and log_p is [b,n,k]. Sum along [k] but mean along [b,n]\n",
        "  return (log_q - log_p).sum(dim=2).mean(dim=(0,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "6zsnjmP7ixV7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4ae8b5ee49384298994f5380b3722d99",
            "7a839406b343411a9b70bfefee27502c",
            "dc9267322fa242608874d993f763f5af",
            "2fd4ec1c5d704e128410bb2563a550df",
            "5a42a3b718c847878fd158c3769e4271",
            "0c05781fdeed453a98fcb8bc08d0bc6b",
            "9e70d2d4416c45fd8753f1814c166819",
            "89700f2db11344919258468e455c61e5",
            "46028d986f8145499ebf6db8b8d74210",
            "7f646955518e43309ea3c6c60dbbb790",
            "6282fea65ddd4660a2712e325e2c636f",
            "90696c74e3ba42338356293ff732b0d7",
            "25471ab6a3b24847ab7849d7a009caf9",
            "f907dd5cc3d84825bdb9ee3b8f1e7c7d",
            "ad955c114d184512a38c67b6e5a34aaa",
            "e59a0579cd4347878ae6016cff095ffc",
            "d7e5ae4a45db4b0aa3eca9c64393c8a5",
            "a107a89d74b14933b6dd47071788e53e",
            "9b25a4daa3ae4d62b562c5243a3383cf",
            "8457afbfa0434866a9a21d0cc723dec2",
            "4754c0b6213c44489e36606b3b0079ad",
            "e2921381767d436e8b408f57a2a5984b"
          ]
        },
        "outputId": "0b69c558-dba2-47cf-aa35-78cd42ddaa06"
      },
      "source": [
        "\n",
        "K_VAE = 2\n",
        "\n",
        "\n",
        "class ConvVAE(nn.Module):\n",
        "  def __init__(self, K, num_filters=32, filter_size=5):\n",
        "    super(ConvVAE, self).__init__()\n",
        "\n",
        "    # With padding=0, the number of pixels cut off from each image dimension\n",
        "    # is filter_size // 2. Double it to get the amount of pixels lost in\n",
        "    # width and height per Conv2D layer, or added back in per\n",
        "    # ConvTranspose2D layer.\n",
        "    filter_reduction = 2 * (filter_size // 2)\n",
        "\n",
        "    # After passing input through two Conv2d layers, the shape will be\n",
        "    # 'shape_after_conv'. This is also the shape that will go into the first\n",
        "    # deconvolution layer in the decoder\n",
        "    self.shape_after_conv = (num_filters,\n",
        "                              data_shape[1]-2*filter_reduction,\n",
        "                              data_shape[2]-2*filter_reduction)\n",
        "    flat_size_after_conv = self.shape_after_conv[0] \\\n",
        "        * self.shape_after_conv[1] \\\n",
        "        * self.shape_after_conv[2]\n",
        "\n",
        "    # Define the recognition model (encoder or q) part\n",
        "    self.q_bias = BiasLayer(data_shape)\n",
        "    self.q_conv_1 = nn.Conv2d(data_shape[0], num_filters, 5)\n",
        "    self.q_conv_2 = nn.Conv2d(num_filters, num_filters, 5)\n",
        "    self.q_flatten = nn.Flatten()\n",
        "    self.q_fc_phi = nn.Linear(flat_size_after_conv, K+1)\n",
        "\n",
        "    # Define the generative model (decoder or p) part\n",
        "    self.p_fc_upsample = nn.Linear(K, flat_size_after_conv)\n",
        "    self.p_unflatten = nn.Unflatten(-1, self.shape_after_conv)\n",
        "    self.p_deconv_1 = nn.ConvTranspose2d(num_filters, num_filters, 5)\n",
        "    self.p_deconv_2 = nn.ConvTranspose2d(num_filters, data_shape[0], 5)\n",
        "    self.p_bias = BiasLayer(data_shape)\n",
        "\n",
        "    # Define a special extra parameter to learn scalar sig_x for all pixels\n",
        "    self.log_sig_x = nn.Parameter(torch.zeros(()))\n",
        "\n",
        "  def infer(self, x):\n",
        "    \"\"\"Map (batch of) x to (batch of) phi which can then be passed to\n",
        "    rsample to get z\n",
        "    \"\"\"\n",
        "    s = self.q_bias(x)\n",
        "    s = F.relu(self.q_conv_1(s))\n",
        "    s = F.relu(self.q_conv_2(s))\n",
        "    flat_s = s.view(s.size()[0], -1)\n",
        "    phi = self.q_fc_phi(flat_s)\n",
        "    return phi\n",
        "\n",
        "  def generate(self, zs):\n",
        "    \"\"\"Map [b,n,k] sized samples of z to [b,n,p] sized images\n",
        "    \"\"\"\n",
        "    # Note that for the purposes of passing through the generator, we need\n",
        "    # to reshape zs to be size [b*n,k]\n",
        "    b, n, k = zs.size()\n",
        "    s = zs.view(b*n, -1)\n",
        "    s = F.relu(self.p_fc_upsample(s)).view((b*n,) + self.shape_after_conv)\n",
        "    s = F.relu(self.p_deconv_1(s))\n",
        "    s = self.p_deconv_2(s)\n",
        "    s = self.p_bias(s)\n",
        "    mu_xs = s.view(b, n, -1)\n",
        "    return mu_xs\n",
        "\n",
        "  def decode(self, zs):\n",
        "    # Included for compatability with conv-AE code\n",
        "    return self.generate(zs.unsqueeze(0))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # VAE.forward() is not used for training, but we'll treat it like a\n",
        "    # classic autoencoder by taking a single sample of z ~ q\n",
        "    phi = self.infer(x)\n",
        "    zs = rsample(phi, 1)\n",
        "    return self.generate(zs).view(x.size())\n",
        "\n",
        "  def elbo(self, x, n=1):\n",
        "    \"\"\"Run input end to end through the VAE and compute the ELBO using n\n",
        "    samples of z\n",
        "    \"\"\"\n",
        "    print('In elbo')\n",
        "    phi = self.infer(x)\n",
        "    print('phi', phi.shape, phi[1])\n",
        "    zs = rsample(phi, n)\n",
        "    print('zs', zs.shape, zs[1])\n",
        "    mu_xs = self.generate(zs)\n",
        "    print('mu',mu_xs.shape, mu_xs[1])\n",
        "    return log_p_x(x, mu_xs, self.log_sig_x.exp()) - kl_q_p(zs, phi)\n",
        "\n",
        "\n",
        "def expected_z(phi):\n",
        "  return phi[:, :-1]\n",
        "\n",
        "\n",
        "def rsample(phi, n_samples):\n",
        "  \"\"\"Sample z ~ q(z;phi)\n",
        "  Ouput z is size [b,n_samples,K] given phi with shape [b,K+1]. The first K\n",
        "  entries of each row of phi are the mean of q, and phi[:,-1] is the log\n",
        "  standard deviation\n",
        "  \"\"\"\n",
        "  print('In rsample')\n",
        "  b, kplus1 = phi.size()\n",
        "  print('b',b,'kplus1',kplus1)\n",
        "  k = kplus1-1\n",
        "  mu, sig = phi[:, :-1], phi[:,-1].exp()\n",
        "  print('mu',mu[1],'sig',sig[1])\n",
        "  eps = torch.randn(b, n_samples, k, device=phi.device)\n",
        "  print('eps',eps[1])\n",
        "  return eps*sig.view(b,1,1) + mu.view(b,1,k)\n",
        "\n",
        "\n",
        "def train_vae(vae, dataset, epochs=10, n_samples=1000):\n",
        "  opt = torch.optim.Adam(vae.parameters(), lr=1e-3, weight_decay=0)\n",
        "  elbo_vals = []\n",
        "  vae.to(DEVICE)\n",
        "  vae.train()\n",
        "  loader = DataLoader(dataset, batch_size=250, shuffle=True, pin_memory=True)\n",
        "  for epoch in trange(epochs, desc='Epochs'):\n",
        "    for im, _ in tqdm(loader, total=len(dataset) // 250, desc='Batches', leave=False):\n",
        "      im = im.to(DEVICE)\n",
        "      im = torch.unsqueeze(im, 1)\n",
        "      print('Im shape',im.shape)\n",
        "      opt.zero_grad()\n",
        "      loss = -vae.elbo(im)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      elbo_vals.append(-loss.item())\n",
        "  vae.to('cpu')\n",
        "  vae.eval()\n",
        "  return elbo_vals\n",
        "\n",
        "\n",
        "trained_conv_VarAE = ConvVAE(K=K_VAE)\n",
        "elbo_vals = train_vae(trained_conv_VarAE, train_dataset, n_samples=10000)\n",
        "\n",
        "print(f'Learned sigma_x is {torch.exp(trained_conv_VarAE.log_sig_x)}')\n",
        "\n",
        "# Uncomment below if you'd like to see the the training\n",
        "# curve of the evaluated ELBO loss function\n",
        "# ELBO is the loss function used to train VAEs (see lecture!)\n",
        "plt.figure()\n",
        "plt.plot(elbo_vals)\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('ELBO')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ae8b5ee49384298994f5380b3722d99",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90696c74e3ba42338356293ff732b0d7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Batches:   0%|          | 0/78 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Im shape torch.Size([250, 1, 128, 129])\n",
            "In elbo\n",
            "phi torch.Size([250, 3]) tensor([57.7423, 16.0517, 66.0804], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "In rsample\n",
            "b 250 kplus1 3\n",
            "mu tensor([57.7423, 16.0517], device='cuda:0', grad_fn=<SelectBackward>) sig tensor(4.9929e+28, device='cuda:0', grad_fn=<SelectBackward>)\n",
            "eps tensor([[-0.8138,  0.3330]], device='cuda:0')\n",
            "zs torch.Size([250, 1, 2]) tensor([[-4.0633e+28,  1.6626e+28]], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "mu torch.Size([250, 1, 16512]) tensor([[-5.5260e+26,  7.4099e+26, -1.4658e+27,  ..., -4.5801e+26,\n",
            "         -1.2067e+27, -9.2506e+26]], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "Im shape torch.Size([250, 1, 128, 129])\n",
            "In elbo\n",
            "phi torch.Size([250, 3]) tensor([nan, nan, nan], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "In rsample\n",
            "b 250 kplus1 3\n",
            "mu tensor([nan, nan], device='cuda:0', grad_fn=<SelectBackward>) sig tensor(nan, device='cuda:0', grad_fn=<SelectBackward>)\n",
            "eps tensor([[-1.2029,  0.7386]], device='cuda:0')\n",
            "zs torch.Size([250, 1, 2]) tensor([[nan, nan]], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "mu torch.Size([250, 1, 16512]) tensor([[nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
            "       grad_fn=<SelectBackward>)\n",
            "Im shape torch.Size([250, 1, 128, 129])\n",
            "In elbo\n",
            "phi torch.Size([250, 3]) tensor([nan, nan, nan], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "In rsample\n",
            "b 250 kplus1 3\n",
            "mu tensor([nan, nan], device='cuda:0', grad_fn=<SelectBackward>) sig tensor(nan, device='cuda:0', grad_fn=<SelectBackward>)\n",
            "eps tensor([[-0.0809,  0.4332]], device='cuda:0')\n",
            "zs torch.Size([250, 1, 2]) tensor([[nan, nan]], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "mu torch.Size([250, 1, 16512]) tensor([[nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
            "       grad_fn=<SelectBackward>)\n",
            "Im shape torch.Size([250, 1, 128, 129])\n",
            "In elbo\n",
            "phi torch.Size([250, 3]) tensor([nan, nan, nan], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "In rsample\n",
            "b 250 kplus1 3\n",
            "mu tensor([nan, nan], device='cuda:0', grad_fn=<SelectBackward>) sig tensor(nan, device='cuda:0', grad_fn=<SelectBackward>)\n",
            "eps tensor([[ 0.8874, -0.8578]], device='cuda:0')\n",
            "zs torch.Size([250, 1, 2]) tensor([[nan, nan]], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "mu torch.Size([250, 1, 16512]) tensor([[nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
            "       grad_fn=<SelectBackward>)\n",
            "Im shape torch.Size([250, 1, 128, 129])\n",
            "In elbo\n",
            "phi torch.Size([250, 3]) tensor([nan, nan, nan], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "In rsample\n",
            "b 250 kplus1 3\n",
            "mu tensor([nan, nan], device='cuda:0', grad_fn=<SelectBackward>) sig tensor(nan, device='cuda:0', grad_fn=<SelectBackward>)\n",
            "eps tensor([[-0.2764, -0.1964]], device='cuda:0')\n",
            "zs torch.Size([250, 1, 2]) tensor([[nan, nan]], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "mu torch.Size([250, 1, 16512]) tensor([[nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
            "       grad_fn=<SelectBackward>)\n",
            "Im shape torch.Size([250, 1, 128, 129])\n",
            "In elbo\n",
            "phi torch.Size([250, 3]) tensor([nan, nan, nan], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "In rsample\n",
            "b 250 kplus1 3\n",
            "mu tensor([nan, nan], device='cuda:0', grad_fn=<SelectBackward>) sig tensor(nan, device='cuda:0', grad_fn=<SelectBackward>)\n",
            "eps tensor([[-0.9002, -0.0250]], device='cuda:0')\n",
            "zs torch.Size([250, 1, 2]) tensor([[nan, nan]], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "mu torch.Size([250, 1, 16512]) tensor([[nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
            "       grad_fn=<SelectBackward>)\n",
            "Im shape torch.Size([250, 1, 128, 129])\n",
            "In elbo\n",
            "phi torch.Size([250, 3]) tensor([nan, nan, nan], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "In rsample\n",
            "b 250 kplus1 3\n",
            "mu tensor([nan, nan], device='cuda:0', grad_fn=<SelectBackward>) sig tensor(nan, device='cuda:0', grad_fn=<SelectBackward>)\n",
            "eps tensor([[-0.5842, -1.9233]], device='cuda:0')\n",
            "zs torch.Size([250, 1, 2]) tensor([[nan, nan]], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "mu torch.Size([250, 1, 16512]) tensor([[nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
            "       grad_fn=<SelectBackward>)\n",
            "Im shape torch.Size([250, 1, 128, 129])\n",
            "In elbo\n",
            "phi torch.Size([250, 3]) tensor([nan, nan, nan], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "In rsample\n",
            "b 250 kplus1 3\n",
            "mu tensor([nan, nan], device='cuda:0', grad_fn=<SelectBackward>) sig tensor(nan, device='cuda:0', grad_fn=<SelectBackward>)\n",
            "eps tensor([[-0.6214,  0.2992]], device='cuda:0')\n",
            "zs torch.Size([250, 1, 2]) tensor([[nan, nan]], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "mu torch.Size([250, 1, 16512]) tensor([[nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
            "       grad_fn=<SelectBackward>)\n",
            "Im shape torch.Size([250, 1, 128, 129])\n",
            "In elbo\n",
            "phi torch.Size([250, 3]) tensor([nan, nan, nan], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "In rsample\n",
            "b 250 kplus1 3\n",
            "mu tensor([nan, nan], device='cuda:0', grad_fn=<SelectBackward>) sig tensor(nan, device='cuda:0', grad_fn=<SelectBackward>)\n",
            "eps tensor([[-0.0532, -1.0927]], device='cuda:0')\n",
            "zs torch.Size([250, 1, 2]) tensor([[nan, nan]], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "mu torch.Size([250, 1, 16512]) tensor([[nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
            "       grad_fn=<SelectBackward>)\n",
            "Im shape torch.Size([250, 1, 128, 129])\n",
            "In elbo\n",
            "phi torch.Size([250, 3]) tensor([nan, nan, nan], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "In rsample\n",
            "b 250 kplus1 3\n",
            "mu tensor([nan, nan], device='cuda:0', grad_fn=<SelectBackward>) sig tensor(nan, device='cuda:0', grad_fn=<SelectBackward>)\n",
            "eps tensor([[0.3242, 0.5213]], device='cuda:0')\n",
            "zs torch.Size([250, 1, 2]) tensor([[nan, nan]], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "mu torch.Size([250, 1, 16512]) tensor([[nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
            "       grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-1f55d51341e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0mtrained_conv_VarAE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK_VAE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0melbo_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_conv_VarAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Learned sigma_x is {torch.exp(trained_conv_VarAE.log_sig_x)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-1f55d51341e9>\u001b[0m in \u001b[0;36mtrain_vae\u001b[0;34m(vae, dataset, epochs, n_samples)\u001b[0m\n\u001b[1;32m    125\u001b[0m       \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m       \u001b[0melbo_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m   \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYlFzRaws6sn",
        "outputId": "067df495-7d9a-4cf1-f083-f6468d32b162"
      },
      "source": [
        "test_3d = torch.zeros([250, 128, 129])\n",
        "\n",
        "test_4d = torch.unsqueeze(test_3d, 1)\n",
        "\n",
        "test_4d.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([250, 1, 128, 129])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-aCtHDptLZh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}