{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tInjwFwaszkK"
   },
   "source": [
    "Until now we have been training the model with png spectrograms, which are colour images and therefor have 3 channels. The colour is just a color map, it doesn't actually mean anything, so we might as well train with the raw spectrogram values. \n",
    "\n",
    "I calculated the spectrograms and saved them all as .npy files, so I'll need to figure out how to get those into the data loader, then change the input channels to 1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NQz2PJsAw-TN"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import os\n",
    "import glob\n",
    "import imageio\n",
    "import random, shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "DkaJYigCxBAr"
   },
   "outputs": [],
   "source": [
    "#Utils\n",
    "\n",
    "def set_device():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"WARNING: For this notebook to perform best, \"\n",
    "          \"if possible, in the menu under `Runtime` -> \"\n",
    "          \"`Change runtime type.`  select `GPU` \")\n",
    "    else:\n",
    "        print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "#  Plotting function.\n",
    "\n",
    "def plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc):\n",
    "    epochs = len(train_loss)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.plot(list(range(epochs)), train_loss, label='Training Loss')\n",
    "    ax1.plot(list(range(epochs)), validation_loss, label='Validation Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Epoch vs Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(list(range(epochs)), train_acc, label='Training Accuracy')\n",
    "    ax2.plot(list(range(epochs)), validation_acc, label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Epoch vs Accuracy')\n",
    "    ax2.legend()\n",
    "    fig.set_size_inches(15.5, 5.5)\n",
    "    plt.show()\n",
    "    \n",
    "def apply_sliding_window(in_dir, out_dir, window_length = 3, hop_length = 1, \n",
    "                         save_as_tensor = False):\n",
    "  #IMPORTANT NOTE - this function assumes that the spectrograms were made with \n",
    "    #default librosa nfft_size, hop_length, etc. \n",
    "\n",
    "    #window and hop length units are in seconds. \n",
    "\n",
    "    window_size = librosa.time_to_frames(window_length)\n",
    "    hop_size = librosa.time_to_frames(hop_length)\n",
    "    #get list of genre folders\n",
    "    files = glob.glob(in_dir + \"*\")\n",
    "    #loop over genre folders\n",
    "        #loop over spectrogram files\n",
    "    for spec_file in files:\n",
    "        #extract file name\n",
    "        file_name = spec_file.split('/')[-1]\n",
    "        file_name = file_name.strip(\".npy\")\n",
    "        #load spectrogram\n",
    "        spec = np.load(spec_file)\n",
    "      \n",
    "        #apply sliding frame to spectrogram\n",
    "        all_frames = librosa.util.frame(spec, window_size, hop_size)\n",
    "        all_frames = np.moveaxis(all_frames, 2, 0)\n",
    "\n",
    "        #loop over individual frames\n",
    "        for i, frame in enumerate(all_frames):\n",
    "        \n",
    "            #specify out file name\n",
    "                if save_as_tensor: #convert np array to tensor\n",
    "                    torch_frame = torch.from_numpy(frame)\n",
    "                    #save torch of spectrogram frame\n",
    "                    torch.save(torch_frame, out_dir + os.path.basename(file_name) + \"_\" + str(i) + \".pt\")  \n",
    "\n",
    "                else: #save spectrogram frame\n",
    "                    np.save(out_dir + os.path.basename(file_name) + \"_\" + str(i), frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2jv-cJsuq7l"
   },
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "XXubASNuuv2R"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-5e4a7fd33f47>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-5e4a7fd33f47>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#@title Default title text\n",
    "\n",
    "fname = \"music.zip\"\n",
    "url = \"https://osf.io/drjhb/download\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.ConnectionError:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "else:\n",
    "    with open(fname, \"wb\") as fid:\n",
    "        fid.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjVUmcijuyAs"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(fname, 'r') as zipObj:\n",
    "# Extract all the contents of zip file in different directory\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE4Xaq8YvIGY"
   },
   "source": [
    "## Make Mel Spectrograms with Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5SPsfpLvLtX",
    "outputId": "945936c7-c2b9-47b4-9e47-8f4d54a80df3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loop\n",
      "blues\n",
      "classical\n",
      "country\n",
      "disco\n",
      "hiphop\n",
      "jazz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipzac\\anaconda3\\lib\\site-packages\\librosa\\core\\audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something is wrong with jazz.00054 so it has been skipped\n",
      "metal\n",
      "pop\n",
      "reggae\n",
      "rock\n"
     ]
    }
   ],
   "source": [
    "in_folder_path = \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\genres_original\\\\\"\n",
    "out_folder_path = \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\spectrograms_30s\\\\\"\n",
    "if not os.path.exists(out_folder_path):\n",
    "    os.mkdir(out_folder_path)\n",
    "\n",
    "#get list of genre folders\n",
    "genre_folders = os.listdir(in_folder_path)\n",
    "\n",
    "#initialize empty list of problem files:\n",
    "problem_files = []\n",
    "\n",
    "#loop over genre folders\n",
    "print('starting loop')\n",
    "for genre_folder in genre_folders: \n",
    "    #get list of individual .wav files\n",
    "    os.chdir(in_folder_path + '\\\\' + genre_folder)\n",
    "    wav_files = os.listdir(\".\")\n",
    "  #get output folder path\n",
    "    out_folder = out_folder_path + '\\\\' + genre_folder\n",
    "  #make output folder if it doesn't exist\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.mkdir(out_folder)\n",
    "\n",
    "  #loop over list of wav files\n",
    "    print(genre_folder)\n",
    "    \n",
    "    for wav_file in wav_files:\n",
    "    #extract file name\n",
    "        file_name = wav_file.split('/')[-1]\n",
    "        file_name = file_name.strip(\".wav\")\n",
    "    \n",
    "    #load wav file\n",
    "        try: \n",
    "            y, sr = librosa.load(wav_file)\n",
    "\n",
    "      #normalize audio\n",
    "            y = (y - y.mean())/ y.std()\n",
    "\n",
    "      #calculate mel spectrogram\n",
    "            spectrogram = librosa.feature.melspectrogram(y, sr = sr)\n",
    "            spectrogram_db = librosa.amplitude_to_db(spectrogram, ref = np.max)\n",
    "\n",
    "      #global normalization of the spectrogram (not frequency band normalization...)\n",
    "            spectrogram_norm = (spectrogram_db - spectrogram_db.mean()) / spectrogram_db.std()\n",
    "\n",
    "      #save spectrogram\n",
    "    \n",
    "            np.save(out_folder + '\\\\' +file_name, spectrogram_norm)\n",
    "        except:\n",
    "            print(\"Something is wrong with \" + file_name + \" so it has been skipped\")\n",
    "            problem_files = problem_files.append(wav_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNq0YxYpyeZM"
   },
   "source": [
    "## Split training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vCeMHisnyV3Y"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\blues\\\\blues.00061.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-5da1528d86e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mg\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mg\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    416\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m     \u001b[0mcopyfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m             \u001b[1;31m# macOS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\blues\\\\blues.00061.npy'"
     ]
    }
   ],
   "source": [
    "## EDIT HERE TO CHANGE SIZE OF TEST AND VAL DATASETS\n",
    "test_prop = 0.15\n",
    "val_prop = 0.15\n",
    "\n",
    "train_prop = 1 - test_prop - val_prop\n",
    "\n",
    "# Create folder with training, testing and validation data.\n",
    "\n",
    "spectrograms_dir = \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\spectrograms_30s\"\n",
    "folder_names = [\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train\\\\\", \n",
    "                \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\", \n",
    "                \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\\"]\n",
    "\n",
    "genres = list(os.listdir(spectrograms_dir))\n",
    "\n",
    "train_dir = folder_names[0]\n",
    "test_dir = folder_names[1]\n",
    "val_dir = folder_names[2]\n",
    "\n",
    "for f in folder_names:\n",
    "    if os.path.exists(f):\n",
    "        shutil.rmtree(f)\n",
    "        os.mkdir(f)\n",
    "    else:\n",
    "        os.mkdir(f)\n",
    "# Loop over all genres\n",
    "for g in genres:\n",
    "    if os.path.exists(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train\\\\\" + g + '\\\\' ):\n",
    "        shutil.rmtree(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train\\\\\" + g + '\\\\' )\n",
    "        os.mkdir(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train\\\\\" + g + '\\\\' )\n",
    "    else:\n",
    "        os.mkdir(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\" + g + '\\\\' )\n",
    "    if os.path.exists(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\" + g + '\\\\' ):\n",
    "        shutil.rmtree(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\" + g + '\\\\' )\n",
    "        os.mkdir(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\" + g + '\\\\' )\n",
    "    else:\n",
    "        os.mkdir(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\\" + g + '\\\\' )\n",
    "    if os.path.exists(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\\" + g + '\\\\' ):\n",
    "        shutil.rmtree(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\\" + g + '\\\\' )\n",
    "        os.mkdir(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\\" + g + '\\\\' )\n",
    "    else:\n",
    "        os.mkdir(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train\\\\\" + g + '\\\\' )\n",
    "        \n",
    "    src_file_paths= glob.glob('C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\spectrograms_30s\\\\' + g + '\\\\*')\n",
    "    # find all images & split in train, test, and validation\n",
    "    random.shuffle(src_file_paths)\n",
    "\n",
    "    test_idx = int(len(src_file_paths) * test_prop)\n",
    "    val_idx = test_idx + int(len(src_file_paths) * val_prop)\n",
    "\n",
    "    test_files = src_file_paths[0:test_idx]\n",
    "    val_files = src_file_paths[test_idx:val_idx]\n",
    "    train_files = src_file_paths[val_idx:]\n",
    "\n",
    "  # copy training and testing images over\n",
    "    for f in train_files:\n",
    "        shutil.copy(f, \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train\\\\\" + g + '\\\\' + os.path.split(f)[-1])\n",
    "    for f in test_files:\n",
    "        shutil.copy(f, \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\" + g  + '\\\\' + os.path.split(f)[-1])\n",
    "    for f in val_files:\n",
    "        shutil.copy(f, \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\\" + g  + '\\\\' + os.path.split(f)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "!copy f \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\" + g  + '\\\\' + os.path.split(f)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZ5eGK1K1wns"
   },
   "source": [
    "## Apply sliding window to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oObuS8PsG6sy",
    "outputId": "d3a067f3-52d2-426b-b155-2ba8535f36a5"
   },
   "outputs": [],
   "source": [
    "cropped_names = [\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train_cropped\\\\\", \n",
    "                \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test_cropped\\\\\", \n",
    "                \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val_cropped\\\\\"]\n",
    "for f in cropped_names:\n",
    "    if os.path.exists(f):\n",
    "        shutil.rmtree(f)\n",
    "        os.mkdir(f)\n",
    "    else:\n",
    "        os.mkdir(f)\n",
    "        \n",
    "# Cut Data\n",
    "\n",
    "apply_sliding_window( \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train\\\\\", \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train_cropped\\\\\", save_as_tensor=True)\n",
    "apply_sliding_window(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test\\\\\", \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\test_cropped\\\\\", save_as_tensor=True)\n",
    "apply_sliding_window(\"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val\\\\\", \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val_cropped\\\\\", save_as_tensor = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvnqIKVNmTrq"
   },
   "source": [
    "## Dataloader - torch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7DNm0yFmXNv"
   },
   "outputs": [],
   "source": [
    "# Data loading.\n",
    "train_dir = \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train_cropped\\\\\"\n",
    "val_dir =  \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val_cropped\\\\\"\n",
    "\n",
    "train_dataset = datasets.DatasetFolder(\n",
    "    train_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)\n",
    "\n",
    "val_dataset = datasets.DatasetFolder(\n",
    "    val_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lmq8RSpJzPn4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data loading.\n",
    "train_dir = \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\train_cropped\\\\\"\n",
    "val_dir =  \"C:\\\\Users\\\\ipzac\\\\Documents\\\\Project Data\\\\Music\\\\Data\\\\val_cropped\\\\\"\n",
    "\n",
    "train_dataset = datasets.DatasetFolder(\n",
    "    train_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load, \n",
    "    transform = transforms.Compose([torchaudio.transforms.TimeMasking(time_mask_param= 40, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param=40, iid_masks=True), \n",
    "                                    torchaudio.transforms.TimeMasking(time_mask_param= 30, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param = 30, iid_masks=True), \n",
    "                                    torchaudio.transforms.TimeMasking(time_mask_param= 20, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param = 20, iid_masks=True)]) )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)\n",
    "\n",
    "val_dataset = datasets.DatasetFolder(\n",
    "    val_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sQgGDQRnSzP"
   },
   "source": [
    "Looks good! And we don't need the squeeze anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AgdXD7RSorg"
   },
   "source": [
    "## Train Default Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LD-455iX5AxJ"
   },
   "outputs": [],
   "source": [
    "# Make a CNN & train it to predict genres.\n",
    "\n",
    "class music_net(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"Intitalize neural net layers\"\"\"\n",
    "    super(music_net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=0) # changed in channels to 1 \n",
    "    self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv3 = nn.Conv2d(in_channels=128, out_channels=512, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv4 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv5 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "    self.fc1 = nn.Linear(in_features=512, out_features=10) \n",
    "    # self.fc2 = nn.Linear(in_features=64, out_features=10)\n",
    "\n",
    "    self.batchnorm1 = nn.BatchNorm2d(num_features=64)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(num_features=128)\n",
    "    self.batchnorm3 = nn.BatchNorm2d(num_features=512)\n",
    "    self.batchnorm4 = nn.BatchNorm2d(num_features=256)\n",
    "    self.batchnorm5 = nn.BatchNorm2d(num_features=128)\n",
    "\n",
    "    self.dropout = nn.Dropout(p=0.5, inplace=False)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Conv layer 1.\n",
    "    x = self.conv1(x)\n",
    "    x = self.batchnorm1(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 2.\n",
    "    x = self.conv2(x)\n",
    "    x = self.batchnorm2(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 3.\n",
    "    x = self.dropout(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.batchnorm3(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 4.\n",
    "    x = self.dropout(x)\n",
    "    x = self.conv4(x)\n",
    "    x = self.batchnorm4(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # # Conv layer 5.\n",
    "    x = self.conv5(x)\n",
    "    x = self.batchnorm5(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    #print(x.shape)################################################################\n",
    "\n",
    "    # Fully connected layer 1.\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.dropout(x)\n",
    "    x = self.fc1(x)\n",
    "    x = F.softmax(x)\n",
    "\n",
    "    # # Fully connected layer 2.\n",
    "    # x = self.dropout(x)\n",
    "    # x = self.fc2(x)\n",
    "    # x = F.softmax(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, validation_loader, epochs):\n",
    "  criterion =  nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0025)\n",
    "  train_loss, validation_loss = [], []\n",
    "  train_acc, validation_acc = [], []\n",
    "  with tqdm(range(epochs), unit='epoch') as tepochs:\n",
    "    tepochs.set_description('Training')\n",
    "    for epoch in tepochs:\n",
    "      model.train()\n",
    "      # keep track of the running loss\n",
    "      running_loss = 0.\n",
    "      correct, total = 0, 0\n",
    "\n",
    "      for data, target in train_loader:\n",
    "        # getting the training set\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        #squeeze out extra dimension in data\n",
    "        #data = data.squeeze(dim = 0) ########################################### this is new. \n",
    "\n",
    "        #using torch dataloader, you have to unsqueeze the data, I think...\n",
    "        data = data.reshape((100, 1, 128, 129))#######  this is hardcoded for now and should be fixed somewhere else. \n",
    "        #100 is the batch size, 1 is for a single channel, and 128 by 129 is the size of the spectrogram image in pixels. \n",
    "\n",
    "        # Get the model output (call the model with the data from this batch)\n",
    "        output = model(data)\n",
    "        # Zero the gradients out)\n",
    "        optimizer.zero_grad()\n",
    "        # Get the Loss\n",
    "        loss  = criterion(output, target)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights (using the training step of the optimizer)\n",
    "        optimizer.step()\n",
    "\n",
    "        tepochs.set_postfix(loss=loss.item())\n",
    "        running_loss += loss  # add the loss for this batch\n",
    "\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "      # append the loss for this epoch\n",
    "      train_loss.append(running_loss/len(train_loader))\n",
    "      train_acc.append(correct/total)\n",
    "\n",
    "      # evaluate on validation data\n",
    "      model.eval()\n",
    "      running_loss = 0.\n",
    "      correct, total = 0, 0\n",
    "\n",
    "      for data, target in validation_loader:\n",
    "        # getting the validation set\n",
    "        #using torch dataloader, you have to unsqueeze the data, I think...\n",
    "        data = data.reshape((100, 1, 128, 129))#######  this is hardcoded for now and should be fixed somewhere else. \n",
    "        #100 is the batch size, 1 is for a single channel, and 128 by 129 is the size of the spectrogram image in pixels. \n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        tepochs.set_postfix(loss=loss.item())\n",
    "        running_loss += loss.item()\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "      validation_loss.append(running_loss/len(validation_loader))\n",
    "      validation_acc.append(correct/total)\n",
    "\n",
    "  return train_loss, train_acc, validation_loss, validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-_PdqDw3cRD"
   },
   "outputs": [],
   "source": [
    "# this is the base code for a variational auto encoder for the MNIst data set\n",
    "# see what modifications will be needed to be done\n",
    "K_VAE = 2\n",
    "\n",
    "\n",
    "class ConvVAE(nn.Module):\n",
    "  def __init__(self, K, num_filters=32, filter_size=5):\n",
    "    super(ConvVAE, self).__init__()\n",
    "\n",
    "    # With padding=0, the number of pixels cut off from each image dimension\n",
    "    # is filter_size // 2. Double it to get the amount of pixels lost in\n",
    "    # width and height per Conv2D layer, or added back in per\n",
    "    # ConvTranspose2D layer.\n",
    "    filter_reduction = 2 * (filter_size // 2)\n",
    "\n",
    "    # After passing input through two Conv2d layers, the shape will be\n",
    "    # 'shape_after_conv'. This is also the shape that will go into the first\n",
    "    # deconvolution layer in the decoder\n",
    "    self.shape_after_conv = (num_filters,\n",
    "                              data_shape[1]-2*filter_reduction,\n",
    "                              data_shape[2]-2*filter_reduction)\n",
    "    flat_size_after_conv = self.shape_after_conv[0] \\\n",
    "        * self.shape_after_conv[1] \\\n",
    "        * self.shape_after_conv[2]\n",
    "\n",
    "    # Define the recognition model (encoder or q) part\n",
    "    self.q_bias = BiasLayer(data_shape)\n",
    "    self.q_conv_1 = nn.Conv2d(data_shape[0], num_filters, 5)\n",
    "    self.q_conv_2 = nn.Conv2d(num_filters, num_filters, 5)\n",
    "    self.q_flatten = nn.Flatten()\n",
    "    self.q_fc_phi = nn.Linear(flat_size_after_conv, K+1)\n",
    "\n",
    "    # Define the generative model (decoder or p) part\n",
    "    self.p_fc_upsample = nn.Linear(K, flat_size_after_conv)\n",
    "    self.p_unflatten = nn.Unflatten(-1, self.shape_after_conv)\n",
    "    self.p_deconv_1 = nn.ConvTranspose2d(num_filters, num_filters, 5)\n",
    "    self.p_deconv_2 = nn.ConvTranspose2d(num_filters, data_shape[0], 5)\n",
    "    self.p_bias = BiasLayer(data_shape)\n",
    "\n",
    "    # Define a special extra parameter to learn scalar sig_x for all pixels\n",
    "    self.log_sig_x = nn.Parameter(torch.zeros(()))\n",
    "\n",
    "  def infer(self, x):\n",
    "    \"\"\"Map (batch of) x to (batch of) phi which can then be passed to\n",
    "    rsample to get z\n",
    "    \"\"\"\n",
    "    s = self.q_bias(x)\n",
    "    s = F.relu(self.q_conv_1(s))\n",
    "    s = F.relu(self.q_conv_2(s))\n",
    "    flat_s = s.view(s.size()[0], -1)\n",
    "    phi = self.q_fc_phi(flat_s)\n",
    "    return phi\n",
    "\n",
    "  def generate(self, zs):\n",
    "    \"\"\"Map [b,n,k] sized samples of z to [b,n,p] sized images\n",
    "    \"\"\"\n",
    "    # Note that for the purposes of passing through the generator, we need\n",
    "    # to reshape zs to be size [b*n,k]\n",
    "    b, n, k = zs.size()\n",
    "    s = zs.view(b*n, -1)\n",
    "    s = F.relu(self.p_fc_upsample(s)).view((b*n,) + self.shape_after_conv)\n",
    "    s = F.relu(self.p_deconv_1(s))\n",
    "    s = self.p_deconv_2(s)\n",
    "    s = self.p_bias(s)\n",
    "    mu_xs = s.view(b, n, -1)\n",
    "    return mu_xs\n",
    "\n",
    "  def decode(self, zs):\n",
    "    # Included for compatability with conv-AE code\n",
    "    return self.generate(zs.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # VAE.forward() is not used for training, but we'll treat it like a\n",
    "    # classic autoencoder by taking a single sample of z ~ q\n",
    "    phi = self.infer(x)\n",
    "    zs = rsample(phi, 1)\n",
    "    return self.generate(zs).view(x.size())\n",
    "\n",
    "  def elbo(self, x, n=1):\n",
    "    \"\"\"Run input end to end through the VAE and compute the ELBO using n\n",
    "    samples of z\n",
    "    \"\"\"\n",
    "    phi = self.infer(x)\n",
    "    zs = rsample(phi, n)\n",
    "    mu_xs = self.generate(zs)\n",
    "    return log_p_x(x, mu_xs, self.log_sig_x.exp()) - kl_q_p(zs, phi)\n",
    "\n",
    "\n",
    "def expected_z(phi):\n",
    "  return phi[:, :-1]\n",
    "\n",
    "\n",
    "def rsample(phi, n_samples):\n",
    "  \"\"\"Sample z ~ q(z;phi)\n",
    "  Ouput z is size [b,n_samples,K] given phi with shape [b,K+1]. The first K\n",
    "  entries of each row of phi are the mean of q, and phi[:,-1] is the log\n",
    "  standard deviation\n",
    "  \"\"\"\n",
    "  b, kplus1 = phi.size()\n",
    "  k = kplus1-1\n",
    "  mu, sig = phi[:, :-1], phi[:,-1].exp()\n",
    "  eps = torch.randn(b, n_samples, k, device=phi.device)\n",
    "  return eps*sig.view(b,1,1) + mu.view(b,1,k)\n",
    "\n",
    "\n",
    "def train_vae(vae, dataset, epochs=10, n_samples=1000):\n",
    "  opt = torch.optim.Adam(vae.parameters(), lr=1e-3, weight_decay=0)\n",
    "  elbo_vals = []\n",
    "  vae.to(DEVICE)\n",
    "  vae.train()\n",
    "  loader = DataLoader(dataset, batch_size=250, shuffle=True, pin_memory=True)\n",
    "  for epoch in trange(epochs, desc='Epochs'):\n",
    "    for im, _ in tqdm(loader, total=len(dataset) // 250, desc='Batches', leave=False):\n",
    "      im = im.to(DEVICE)\n",
    "      opt.zero_grad()\n",
    "      loss = -vae.elbo(im)\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "\n",
    "      elbo_vals.append(-loss.item())\n",
    "  vae.to('cpu')\n",
    "  vae.eval()\n",
    "  return elbo_vals\n",
    "\n",
    "\n",
    "# trained_conv_VarAE = ConvVAE(K=K_VAE)\n",
    "# elbo_vals = train_vae(trained_conv_VarAE, train_set, n_samples=10000)\n",
    "\n",
    "# print(f'Learned sigma_x is {torch.exp(trained_conv_VarAE.log_sig_x)}')\n",
    "\n",
    "# Uncomment below if you'd like to see the the training\n",
    "# curve of the evaluated ELBO loss function\n",
    "# ELBO is the loss function used to train VAEs (see lecture!)\n",
    "# plt.figure()\n",
    "# plt.plot(elbo_vals)\n",
    "# plt.xlabel('Batch #')\n",
    "# plt.ylabel('ELBO')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "0h96moK463Y8",
    "outputId": "5cdb693b-80ec-4b01-f2ef-a408ee2b21d0"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "net = ConvVAE(K_VAE).to(device)\n",
    "elbo_vals = train_vae(net, train_loader, ephocs = 5, n_samples = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxljFrGg6hEC"
   },
   "outputs": [],
   "source": [
    "# Run training.\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 30)\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwFcIYEy57_k"
   },
   "source": [
    "# Run training.\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 30)\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zeaz6LQg-Sde"
   },
   "source": [
    "Very interesting! We reach high training accuracy in much fewer epochs here than we did with the original png dataset. I should note that I am also using a larger batch size here than there. Could that be having an effect? \n",
    "\n",
    "We reach that high accuracy on the training set without any of the 'bumps' that we saw when training on the png dataset for longer. Maybe that's a sign that I should stop focusing so much on the RSA and instead focus on reducing the overfitting with this model. \n",
    "\n",
    "Some ideas on reducing the overfitting: \n",
    "- Data augmentation\n",
    "- More dropout\n",
    "- L1 / L2 reglularization (doesn't seem to be included at the moment). \n",
    "- Reduce the complexity of the model (fewer layers, narrower layers). \n",
    "- early stopping (although based on the plots, that doesn't seem like it would make a difference.) \n",
    "\n",
    "The validation accuracy is also very noisy. I already made the validation set larger than before (15% instead of 10%). Is there anything else we can do about that? Is it something we should be worried about? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6u-6c5nzhHJ"
   },
   "source": [
    "# Experimenting with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XviWncL0zkGM"
   },
   "source": [
    "Robert suggested that a good place to start would be simplifying the model. Strip it down to as few layers as possible until it can barely improve on the training set, then add complexity back in, looking for a sweetspot with good fit without massive overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9ozJ2dozsvI"
   },
   "source": [
    "## Reducing number of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR45bico1Vkb"
   },
   "source": [
    "# Alex's Advice on Regularization: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KITOaNu-bFEJ"
   },
   "source": [
    "Here are some ideas of changes to the model with Alex's help: \n",
    "\n",
    "- Increase the stride to decrease computation time (thanks Zach!)\n",
    "\n",
    "- Dropout after conv2D layer (thanks Courtnie!)\n",
    "\n",
    "- Create a bottleneck shape for the channels (wide to narrow to wide) (thanks Alex!)\n",
    "  - for example, instead of [1, 8, 32, 64, 128, 512] do [1, 64, 512, 128]. \n",
    "\n",
    "- Use only a subset of the training data for hyperparameter setting. That will make this iteration process faster. (thanks Alex!)\n",
    "\n",
    "- Try increasing stride instead of max pooling. (thanks Alex)\n",
    "\n",
    "- Play with learning rate a bit , but this is lower priority. (thanks Alex). \n",
    "  - Try a slightly lower learning rate. \n",
    "\n",
    "- More data augmentation should be priority #1. \n",
    "\n",
    "- With optimizers other than adam (simpler ones) learning rate decay usually improves accuracy. With adam it isn't so necessary. \n",
    "  - There are some cases where adam doesn't work so great. Hard to predict what those cases will be. Sometimes SGD + learning rate decay is better. It is usually about equally good to adam, but adam is simpler to set up because fewer parameters. \n",
    "\n",
    "-  Our next meeting with Alex will be on Thursday- same time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsHy0F4UiaMJ"
   },
   "source": [
    "Here are some ideas about augmentation in a dataloader with Alex: \n",
    "\n",
    "- Most direct option is to write a dataloader that applies the windowing, the augmentation, the spectrograming, all at the dataloader phase. \n",
    "\n",
    "- Another option would be to apply the transformation in the frequency domain, so that we apply the augmentations on the spectrograms, not the signal. \n",
    "  - this might be the better option, because it will be faster (don't need to fft every time). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQXQpW-lFQ7z"
   },
   "source": [
    "### Bottle Neck Shape of Network\n",
    "\n",
    "I'm going to use the same number of layers as the original network, but make the filter size get wide --> small --> big. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0xTFO3iFfLT"
   },
   "outputs": [],
   "source": [
    "# Make a CNN & train it to predict genres.\n",
    "\n",
    "class music_net(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"Intitalize neural net layers\"\"\"\n",
    "    super(music_net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=0) # changed in channels to 1 \n",
    "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv4 = nn.Conv2d(in_channels=128, out_channels=512, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv5 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "    self.fc1 = nn.Linear(in_features=512, out_features=10) ##################################### changed in_features\n",
    "\n",
    "    self.batchnorm1 = nn.BatchNorm2d(num_features=16)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(num_features=64)\n",
    "    self.batchnorm3 = nn.BatchNorm2d(num_features=128)\n",
    "    self.batchnorm4 = nn.BatchNorm2d(num_features=512)\n",
    "    self.batchnorm5 = nn.BatchNorm2d(num_features=128)\n",
    "\n",
    "    self.dropout = nn.Dropout(p=0.3, inplace=False)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Conv layer 1.\n",
    "    x = self.conv1(x)\n",
    "    x = self.batchnorm1(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 2.\n",
    "    x = self.conv2(x)\n",
    "    x = self.batchnorm2(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 3.\n",
    "    x = self.conv3(x)\n",
    "    x = self.batchnorm3(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 4.\n",
    "    x = self.conv4(x)\n",
    "    x = self.batchnorm4(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 5.\n",
    "    x = self.conv5(x)\n",
    "    x = self.batchnorm5(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    #print(x.shape)################################################################\n",
    "\n",
    "    # Fully connected layer 1.\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.dropout(x)\n",
    "    x = self.fc1(x)\n",
    "    x = F.softmax(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, validation_loader, epochs):\n",
    "  criterion =  nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "  train_loss, validation_loss = [], []\n",
    "  train_acc, validation_acc = [], []\n",
    "  with tqdm(range(epochs), unit='epoch') as tepochs:\n",
    "    tepochs.set_description('Training')\n",
    "    for epoch in tepochs:\n",
    "      model.train()\n",
    "      # keep track of the running loss\n",
    "      running_loss = 0.\n",
    "      correct, total = 0, 0\n",
    "\n",
    "      for data, target in train_loader:\n",
    "        # getting the training set\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        #squeeze out extra dimension in data\n",
    "        #data = data.squeeze(dim = 0) ########################################### this is new. \n",
    "\n",
    "        #using torch dataloader, you have to unsqueeze the data, I think...\n",
    "        data = data.reshape((100, 1, 128, 129))#######  this is hardcoded for now and should be fixed somewhere else. \n",
    "        #100 is the batch size, 1 is for a single channel, and 128 by 129 is the size of the spectrogram image in pixels. \n",
    "\n",
    "        # Get the model output (call the model with the data from this batch)\n",
    "        output = model(data)\n",
    "        # Zero the gradients out)\n",
    "        optimizer.zero_grad()\n",
    "        # Get the Loss\n",
    "        loss  = criterion(output, target)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights (using the training step of the optimizer)\n",
    "        optimizer.step()\n",
    "\n",
    "        tepochs.set_postfix(loss=loss.item())\n",
    "        running_loss += loss  # add the loss for this batch\n",
    "\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "      # append the loss for this epoch\n",
    "      train_loss.append(running_loss/len(train_loader))\n",
    "      train_acc.append(correct/total)\n",
    "\n",
    "      # evaluate on validation data\n",
    "      model.eval()\n",
    "      running_loss = 0.\n",
    "      correct, total = 0, 0\n",
    "\n",
    "      for data, target in validation_loader:\n",
    "        # getting the validation set\n",
    "        #using torch dataloader, you have to unsqueeze the data, I think...\n",
    "        data = data.reshape((100, 1, 128, 129))#######  this is hardcoded for now and should be fixed somewhere else. \n",
    "        #100 is the batch size, 1 is for a single channel, and 128 by 129 is the size of the spectrogram image in pixels. \n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        tepochs.set_postfix(loss=loss.item())\n",
    "        running_loss += loss.item()\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "      validation_loss.append(running_loss/len(validation_loader))\n",
    "      validation_acc.append(correct/total)\n",
    "\n",
    "  return train_loss, train_acc, validation_loss, validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475,
     "referenced_widgets": [
      "2f5a538b5bad4070a29f49079ea60615",
      "eeb5b2ca066b4a198ebf3e9a7da59cf4",
      "75120dc7a96a4420bd1d4f81cafbb677",
      "44c07e071e5a4e41a2364597439b00cb",
      "424b11fd66e74ec584db03c70fe15a7b",
      "7857881ed4784d86ab5447e4d123cc33",
      "b83c8f4d01a4419d8e27e5a308b39d1b",
      "932c752fd5fb42f4a714f8fc372d08cb"
     ]
    },
    "id": "OLxUNlToGPnb",
    "outputId": "4e04b4a5-caa3-4279-d6c5-2fd3892dd2ea"
   },
   "outputs": [],
   "source": [
    "# Run training.\n",
    "\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 50)\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sBaBe_yGSaU"
   },
   "source": [
    "That did help it learn the training set faster, compared to a network with the same number of layers but the size was doubling at every layer, but it didn't change much with the validation set accuracy. Maybe I'll stick with this architecture and try applying other regularization tricks to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-aa2-ZoJ63p"
   },
   "source": [
    "### Data Augmentation with SpecAugment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zncnEZv3J97L"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFWKKNkFKHHI"
   },
   "outputs": [],
   "source": [
    "# Data loading.\n",
    "train_dir = \"/content/train_cropped/\"\n",
    "val_dir =  \"/content/val_cropped/\"\n",
    "\n",
    "train_dataset = datasets.DatasetFolder(\n",
    "    train_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load, \n",
    "    transform = transforms.Compose([torchaudio.transforms.TimeMasking(time_mask_param= 10, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param=5, iid_masks=True)]) )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)\n",
    "\n",
    "val_dataset = datasets.DatasetFolder(\n",
    "    val_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lmps3SHZKrS6"
   },
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpfJ8cypLj1A"
   },
   "outputs": [],
   "source": [
    "for data, target in train_dataset:\n",
    "  print(target)\n",
    "  print(data.shape)\n",
    "  plt.figure()\n",
    "  plt.imshow(data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441,
     "referenced_widgets": [
      "e552dfa912344996a8835f468d1f27d3",
      "24c382687bca41c4bab00171a7d36831",
      "7183265a3d0f4baf95a4007076a41980",
      "bf6047816f904ba681f338e2bb6d655a",
      "8ee7e3462caa4ad5bab4253d08daa171",
      "7390849f069e4bc892baa0c7e25c6188",
      "048df7632d1642c6b2570fe1840069eb",
      "a71598188af648b8beb7d763ea05c710"
     ]
    },
    "id": "eDKs7C1zMgXI",
    "outputId": "dcb3dc1d-617b-4a42-8d2c-f92741b46057"
   },
   "outputs": [],
   "source": [
    "# Run training.\n",
    "\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 50)\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsMtKxDEM3nS"
   },
   "source": [
    "It seems that that level of masking really didn't have much effect at all. Maybe we need wider masking bands? I don't know how much we can push it really. I'll try again with much larger masks and fewer epochs just to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttTdI5DWRPzi"
   },
   "outputs": [],
   "source": [
    "# Data loading.\n",
    "train_dir = \"/content/train_cropped/\"\n",
    "val_dir =  \"/content/val_cropped/\"\n",
    "\n",
    "train_dataset = datasets.DatasetFolder(\n",
    "    train_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load, \n",
    "    transform = transforms.Compose([torchaudio.transforms.TimeMasking(time_mask_param= 20, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param=20, iid_masks=True), \n",
    "                                    torchaudio.transforms.TimeMasking(time_mask_param= 20, iid_masks=True)]) )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)\n",
    "\n",
    "val_dataset = datasets.DatasetFolder(\n",
    "    val_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475,
     "referenced_widgets": [
      "903d46ce405e49688fe86e8091c3075d",
      "14a1d55b468842de91527b6ea7474212",
      "ad86d462f45f4f899f79b3526039732d",
      "a54a239906e54b55a7febfe3c67b3dd8",
      "c5be2588bf1e4551a65f601812ba59b9",
      "b109d0bb482846889d3f2b10ca6b9dc4",
      "ae6aa230b12e48b48dfadd974dd84e25",
      "a67e6142bb5a481eb6f7fb3e7e92e68d"
     ]
    },
    "id": "WO7knWhkRbKd",
    "outputId": "932ff924-8093-4a40-cccb-09bc6ef58a9e"
   },
   "outputs": [],
   "source": [
    "# Run training.\n",
    "\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 50)\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJcqUjQOReYb"
   },
   "source": [
    "This does seem to have helped. The validation accuracy is now hovering around the mid 70s, not the low 70s it seems. Maybe we can push the masking even further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBBKTbT4SnwL"
   },
   "outputs": [],
   "source": [
    "# Data loading.\n",
    "train_dir = \"/content/train_cropped/\"\n",
    "val_dir =  \"/content/val_cropped/\"\n",
    "\n",
    "train_dataset = datasets.DatasetFolder(\n",
    "    train_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load, \n",
    "    transform = transforms.Compose([torchaudio.transforms.TimeMasking(time_mask_param= 40, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param=40, iid_masks=True), \n",
    "                                    torchaudio.transforms.TimeMasking(time_mask_param= 20, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param = 20, iid_masks=True)]) )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)\n",
    "\n",
    "val_dataset = datasets.DatasetFolder(\n",
    "    val_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499,
     "referenced_widgets": [
      "6905fce7caed4c8eb6b9132456b7ce83",
      "3fc6010ddb0a4343957259a35d409bde",
      "4dbcccdd47494b0080a3ddb58f78db79",
      "add790de876f40a4a776b6bf510ec7ae",
      "70d90a18ab5b4325ad6d775448fdc9f5",
      "cbe0162577a5424b90d2a0d1dc9c4dee",
      "f588fbb7090d4c7b82eee080c1c6280c",
      "a94ecc5031594ce3b165c1221373a361"
     ]
    },
    "id": "tzaO-JJNTTVO",
    "outputId": "3c76bf64-266f-4643-f0c8-09ba5dbccfa3"
   },
   "outputs": [],
   "source": [
    "# Run training.\n",
    "\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 50)\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1eown8uTVZG"
   },
   "source": [
    "Looks like that is going even better! I'm not to increase the augmentation ***even more*** and also increase the learning rate slightly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-2_iyTOpbHKS"
   },
   "outputs": [],
   "source": [
    "# Data loading.\n",
    "train_dir = \"/content/train_cropped/\"\n",
    "val_dir =  \"/content/val_cropped/\"\n",
    "\n",
    "train_dataset = datasets.DatasetFolder(\n",
    "    train_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load, \n",
    "    transform = transforms.Compose([torchaudio.transforms.TimeMasking(time_mask_param= 40, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param=40, iid_masks=True), \n",
    "                                    torchaudio.transforms.TimeMasking(time_mask_param= 30, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param = 30, iid_masks=True), \n",
    "                                    torchaudio.transforms.TimeMasking(time_mask_param= 20, iid_masks=True), \n",
    "                                    torchaudio.transforms.FrequencyMasking(freq_mask_param = 20, iid_masks=True)]) )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)\n",
    "\n",
    "val_dataset = datasets.DatasetFolder(\n",
    "    val_dir, \n",
    "    extensions = (\".pt\"), \n",
    "    loader = torch.load )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=100, shuffle=True, num_workers=0, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05hoh57SbRGi"
   },
   "outputs": [],
   "source": [
    "# Make a CNN & train it to predict genres.\n",
    "\n",
    "class music_net(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"Intitalize neural net layers\"\"\"\n",
    "    super(music_net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=0) # changed in channels to 1 \n",
    "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv4 = nn.Conv2d(in_channels=128, out_channels=512, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv5 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "    self.fc1 = nn.Linear(in_features=512, out_features=10) ##################################### changed in_features\n",
    "\n",
    "    self.batchnorm1 = nn.BatchNorm2d(num_features=16)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(num_features=64)\n",
    "    self.batchnorm3 = nn.BatchNorm2d(num_features=128)\n",
    "    self.batchnorm4 = nn.BatchNorm2d(num_features=512)\n",
    "    self.batchnorm5 = nn.BatchNorm2d(num_features=128)\n",
    "\n",
    "    self.dropout = nn.Dropout(p=0.3, inplace=False)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Conv layer 1.\n",
    "    x = self.conv1(x)\n",
    "    x = self.batchnorm1(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 2.\n",
    "    x = self.conv2(x)\n",
    "    x = self.batchnorm2(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 3.\n",
    "    x = self.conv3(x)\n",
    "    x = self.batchnorm3(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 4.\n",
    "    x = self.conv4(x)\n",
    "    x = self.batchnorm4(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 5.\n",
    "    x = self.conv5(x)\n",
    "    x = self.batchnorm5(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    #print(x.shape)################################################################\n",
    "\n",
    "    # Fully connected layer 1.\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.dropout(x)\n",
    "    x = self.fc1(x)\n",
    "    x = F.softmax(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, validation_loader, epochs):\n",
    "  criterion =  nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "  train_loss, validation_loss = [], []\n",
    "  train_acc, validation_acc = [], []\n",
    "  with tqdm(range(epochs), unit='epoch') as tepochs:\n",
    "    tepochs.set_description('Training')\n",
    "    for epoch in tepochs:\n",
    "      model.train()\n",
    "      # keep track of the running loss\n",
    "      running_loss = 0.\n",
    "      correct, total = 0, 0\n",
    "\n",
    "      for data, target in train_loader:\n",
    "        # getting the training set\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        #squeeze out extra dimension in data\n",
    "        #data = data.squeeze(dim = 0) ########################################### this is new. \n",
    "\n",
    "        #using torch dataloader, you have to unsqueeze the data, I think...\n",
    "        data = data.reshape((100, 1, 128, 129))#######  this is hardcoded for now and should be fixed somewhere else. \n",
    "        #100 is the batch size, 1 is for a single channel, and 128 by 129 is the size of the spectrogram image in pixels. \n",
    "\n",
    "        # Get the model output (call the model with the data from this batch)\n",
    "        output = model(data)\n",
    "        # Zero the gradients out)\n",
    "        optimizer.zero_grad()\n",
    "        # Get the Loss\n",
    "        loss  = criterion(output, target)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights (using the training step of the optimizer)\n",
    "        optimizer.step()\n",
    "\n",
    "        tepochs.set_postfix(loss=loss.item())\n",
    "        running_loss += loss  # add the loss for this batch\n",
    "\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "      # append the loss for this epoch\n",
    "      train_loss.append(running_loss/len(train_loader))\n",
    "      train_acc.append(correct/total)\n",
    "\n",
    "      # evaluate on validation data\n",
    "      model.eval()\n",
    "      running_loss = 0.\n",
    "      correct, total = 0, 0\n",
    "\n",
    "      for data, target in validation_loader:\n",
    "        # getting the validation set\n",
    "        #using torch dataloader, you have to unsqueeze the data, I think...\n",
    "        data = data.reshape((100, 1, 128, 129))#######  this is hardcoded for now and should be fixed somewhere else. \n",
    "        #100 is the batch size, 1 is for a single channel, and 128 by 129 is the size of the spectrogram image in pixels. \n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        tepochs.set_postfix(loss=loss.item())\n",
    "        running_loss += loss.item()\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "      validation_loss.append(running_loss/len(validation_loader))\n",
    "      validation_acc.append(correct/total)\n",
    "\n",
    "  return train_loss, train_acc, validation_loss, validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "4c9d01d192c34f67922e4566d0474d74",
      "693a904bc74c4661a26bb4352e5a3335",
      "93745946cc8d40d09992ca9e8a205243",
      "e686e8dd897f4509961cb99e2cbc9f0c",
      "1724869d5fa04d208b4b574311e95c8c",
      "d420664ff4604f3fb976b86a4e6e59c4",
      "73e6dcbae47d4cda9bcf8503920f7305",
      "a0228209059c49f8b19bdf17ed2c0f87"
     ]
    },
    "id": "bMran6yJbdoD",
    "outputId": "6a69c43e-1072-41b3-fc03-fc9d507bb642"
   },
   "outputs": [],
   "source": [
    "# Run training.\n",
    "\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 50)\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkRF0bVgbhmA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "210816_ZI__music_classify_",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "048df7632d1642c6b2570fe1840069eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14a1d55b468842de91527b6ea7474212": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1724869d5fa04d208b4b574311e95c8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "24c382687bca41c4bab00171a7d36831": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f5a538b5bad4070a29f49079ea60615": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_75120dc7a96a4420bd1d4f81cafbb677",
       "IPY_MODEL_44c07e071e5a4e41a2364597439b00cb"
      ],
      "layout": "IPY_MODEL_eeb5b2ca066b4a198ebf3e9a7da59cf4"
     }
    },
    "3fc6010ddb0a4343957259a35d409bde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "424b11fd66e74ec584db03c70fe15a7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "44c07e071e5a4e41a2364597439b00cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_932c752fd5fb42f4a714f8fc372d08cb",
      "placeholder": "​",
      "style": "IPY_MODEL_b83c8f4d01a4419d8e27e5a308b39d1b",
      "value": " 50/50 [24:42&lt;00:00, 29.65s/epoch, loss=1.74]"
     }
    },
    "4c9d01d192c34f67922e4566d0474d74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_93745946cc8d40d09992ca9e8a205243",
       "IPY_MODEL_e686e8dd897f4509961cb99e2cbc9f0c"
      ],
      "layout": "IPY_MODEL_693a904bc74c4661a26bb4352e5a3335"
     }
    },
    "4dbcccdd47494b0080a3ddb58f78db79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Training: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbe0162577a5424b90d2a0d1dc9c4dee",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_70d90a18ab5b4325ad6d775448fdc9f5",
      "value": 50
     }
    },
    "6905fce7caed4c8eb6b9132456b7ce83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4dbcccdd47494b0080a3ddb58f78db79",
       "IPY_MODEL_add790de876f40a4a776b6bf510ec7ae"
      ],
      "layout": "IPY_MODEL_3fc6010ddb0a4343957259a35d409bde"
     }
    },
    "693a904bc74c4661a26bb4352e5a3335": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70d90a18ab5b4325ad6d775448fdc9f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7183265a3d0f4baf95a4007076a41980": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Training: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7390849f069e4bc892baa0c7e25c6188",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8ee7e3462caa4ad5bab4253d08daa171",
      "value": 50
     }
    },
    "7390849f069e4bc892baa0c7e25c6188": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73e6dcbae47d4cda9bcf8503920f7305": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75120dc7a96a4420bd1d4f81cafbb677": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Training: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7857881ed4784d86ab5447e4d123cc33",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_424b11fd66e74ec584db03c70fe15a7b",
      "value": 50
     }
    },
    "7857881ed4784d86ab5447e4d123cc33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ee7e3462caa4ad5bab4253d08daa171": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "903d46ce405e49688fe86e8091c3075d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ad86d462f45f4f899f79b3526039732d",
       "IPY_MODEL_a54a239906e54b55a7febfe3c67b3dd8"
      ],
      "layout": "IPY_MODEL_14a1d55b468842de91527b6ea7474212"
     }
    },
    "932c752fd5fb42f4a714f8fc372d08cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93745946cc8d40d09992ca9e8a205243": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "Training:   0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d420664ff4604f3fb976b86a4e6e59c4",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1724869d5fa04d208b4b574311e95c8c",
      "value": 0
     }
    },
    "a0228209059c49f8b19bdf17ed2c0f87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a54a239906e54b55a7febfe3c67b3dd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a67e6142bb5a481eb6f7fb3e7e92e68d",
      "placeholder": "​",
      "style": "IPY_MODEL_ae6aa230b12e48b48dfadd974dd84e25",
      "value": " 50/50 [19:03&lt;00:00, 22.87s/epoch, loss=1.74]"
     }
    },
    "a67e6142bb5a481eb6f7fb3e7e92e68d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a71598188af648b8beb7d763ea05c710": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a94ecc5031594ce3b165c1221373a361": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad86d462f45f4f899f79b3526039732d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Training: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b109d0bb482846889d3f2b10ca6b9dc4",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c5be2588bf1e4551a65f601812ba59b9",
      "value": 50
     }
    },
    "add790de876f40a4a776b6bf510ec7ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a94ecc5031594ce3b165c1221373a361",
      "placeholder": "​",
      "style": "IPY_MODEL_f588fbb7090d4c7b82eee080c1c6280c",
      "value": " 50/50 [27:40&lt;00:00, 33.20s/epoch, loss=1.77]"
     }
    },
    "ae6aa230b12e48b48dfadd974dd84e25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b109d0bb482846889d3f2b10ca6b9dc4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b83c8f4d01a4419d8e27e5a308b39d1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf6047816f904ba681f338e2bb6d655a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a71598188af648b8beb7d763ea05c710",
      "placeholder": "​",
      "style": "IPY_MODEL_048df7632d1642c6b2570fe1840069eb",
      "value": " 50/50 [23:46&lt;00:00, 28.53s/epoch, loss=1.75]"
     }
    },
    "c5be2588bf1e4551a65f601812ba59b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cbe0162577a5424b90d2a0d1dc9c4dee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d420664ff4604f3fb976b86a4e6e59c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e552dfa912344996a8835f468d1f27d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7183265a3d0f4baf95a4007076a41980",
       "IPY_MODEL_bf6047816f904ba681f338e2bb6d655a"
      ],
      "layout": "IPY_MODEL_24c382687bca41c4bab00171a7d36831"
     }
    },
    "e686e8dd897f4509961cb99e2cbc9f0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0228209059c49f8b19bdf17ed2c0f87",
      "placeholder": "​",
      "style": "IPY_MODEL_73e6dcbae47d4cda9bcf8503920f7305",
      "value": " 0/50 [00:27&lt;?, ?epoch/s, loss=2.01]"
     }
    },
    "eeb5b2ca066b4a198ebf3e9a7da59cf4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f588fbb7090d4c7b82eee080c1c6280c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
