{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b8d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision as tv\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "EPS = 1e-6\n",
    "ALPHA_RECONSTRUCT_IMAGE = 1\n",
    "ALPHA_RECONSTRUCT_LATENT = 0.5\n",
    "ALPHA_DISCRIMINATE_IMAGE = 0.005\n",
    "ALPHA_DISCRIMINATE_LATENT = 0.1\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"A generator for mapping a latent space to a sample space.\n",
    "    Input shape: (?, latent_dim)\n",
    "    Output shape: (?, 3, 96, 96)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int = 16):\n",
    "        \"\"\"Initialize generator.\n",
    "        Args:\n",
    "            latent_dim (int): latent dimension (\"noise vector\")\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self._init_modules()\n",
    "\n",
    "    def build_colourspace(self, input_dim: int, output_dim: int):\n",
    "        \"\"\"Build a small module for selecting colours.\"\"\"\n",
    "        colourspace = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                input_dim,\n",
    "                128,\n",
    "                bias=True),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(\n",
    "                128,\n",
    "                64,\n",
    "                bias=True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(\n",
    "                64,\n",
    "                output_dim,\n",
    "                bias=True),\n",
    "            nn.Tanh(),\n",
    "            )\n",
    "        return colourspace\n",
    "\n",
    "    def _init_modules(self):\n",
    "        \"\"\"Initialize the modules.\"\"\"\n",
    "        projection_widths = [8, 8, 8, 8, 8, 8, 8]\n",
    "        self.projection_dim = sum(projection_widths) + self.latent_dim\n",
    "        self.projection = nn.ModuleList()\n",
    "        for index, i in enumerate(projection_widths):\n",
    "            self.projection.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(\n",
    "                        self.latent_dim + sum(projection_widths[:index]),\n",
    "                        i,\n",
    "                        bias=True,\n",
    "                        ),\n",
    "                    nn.BatchNorm1d(8),\n",
    "                    nn.LeakyReLU(),\n",
    "                    )\n",
    "                )\n",
    "        self.projection_upscaler = nn.Upsample(scale_factor=3)\n",
    "\n",
    "        self.colourspace_r = self.build_colourspace(self.projection_dim, 16)\n",
    "        self.colourspace_g = self.build_colourspace(self.projection_dim, 16)\n",
    "        self.colourspace_b = self.build_colourspace(self.projection_dim, 16)\n",
    "        self.colourspace_upscaler = nn.Upsample(scale_factor=96)\n",
    "\n",
    "        self.seed = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                self.projection_dim,\n",
    "                512*3*3,\n",
    "                bias=True),\n",
    "            nn.BatchNorm1d(512*3*3),\n",
    "            nn.LeakyReLU(),\n",
    "            )\n",
    "\n",
    "        self.upscaling = nn.ModuleList()\n",
    "        self.conv = nn.ModuleList()\n",
    "\n",
    "        self.upscaling.append(nn.Upsample(scale_factor=2))\n",
    "        self.conv.append(nn.Sequential(\n",
    "            nn.ZeroPad2d((1, 1, 1, 1)),\n",
    "            nn.Conv2d(\n",
    "                in_channels=(512)//4,\n",
    "                out_channels=512,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=True\n",
    "                ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            ))\n",
    "\n",
    "        self.upscaling.append(nn.Upsample(scale_factor=2))\n",
    "        self.conv.append(nn.Sequential(\n",
    "            nn.ZeroPad2d((1, 2, 1, 2)),\n",
    "            nn.Conv2d(\n",
    "                in_channels=(512 + self.projection_dim)//4,\n",
    "                out_channels=256,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=True\n",
    "                ),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            ))\n",
    "\n",
    "        self.upscaling.append(nn.Upsample(scale_factor=2))\n",
    "        self.conv.append(nn.Sequential(\n",
    "            nn.ZeroPad2d((1, 2, 1, 2)),\n",
    "            nn.Conv2d(\n",
    "                in_channels=(256 + self.projection_dim)//4,\n",
    "                out_channels=256,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=True\n",
    "                ),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            ))\n",
    "\n",
    "        self.upscaling.append(nn.Upsample(scale_factor=2))\n",
    "        self.conv.append(nn.Sequential(\n",
    "            nn.ZeroPad2d((1, 2, 1, 2)),\n",
    "            nn.Conv2d(\n",
    "                in_channels=(256 + self.projection_dim)//4,\n",
    "                out_channels=256,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=True\n",
    "                ),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            )),\n",
    "\n",
    "        self.upscaling.append(nn.Upsample(scale_factor=2))\n",
    "        self.conv.append(nn.Sequential(\n",
    "            nn.ZeroPad2d((1, 2, 1, 2)),\n",
    "            nn.Conv2d(\n",
    "                in_channels=(256 + self.projection_dim)//4,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=True\n",
    "                ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            ))\n",
    "\n",
    "        self.upscaling.append(nn.Upsample(scale_factor=1))\n",
    "        self.conv.append(nn.Sequential(\n",
    "            nn.ZeroPad2d((2, 2, 2, 2)),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=16,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=True\n",
    "                ),\n",
    "            nn.Softmax(dim=1),\n",
    "            ))\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
    "        last = input_tensor\n",
    "        for module in self.projection:\n",
    "            projection = module(last)\n",
    "            last = torch.cat((last, projection), -1)\n",
    "        projection = last\n",
    "\n",
    "        intermediate = self.seed(projection)\n",
    "        intermediate = intermediate.view((-1, 512, 3, 3))\n",
    "\n",
    "        projection_2d = projection.view((-1, self.projection_dim, 1, 1))\n",
    "        projection_2d = self.projection_upscaler(projection_2d)\n",
    "\n",
    "        for i, (conv, upscaling) in enumerate(zip(self.conv, self.upscaling)):\n",
    "            if i + 1 != len(self.upscaling):\n",
    "                if i > 0:\n",
    "                    intermediate = torch.cat((intermediate, projection_2d), 1)\n",
    "                intermediate = torch.nn.functional.pixel_shuffle(intermediate, 2)\n",
    "            intermediate = conv(intermediate)\n",
    "            projection_2d = upscaling(projection_2d)\n",
    "\n",
    "        r_space = self.colourspace_r(projection)\n",
    "        r_space = r_space.view((-1, 16, 1, 1))\n",
    "        r_space = self.colourspace_upscaler(r_space)\n",
    "        r_space = intermediate * r_space\n",
    "        r_space = torch.sum(r_space, dim=1, keepdim=True)\n",
    "\n",
    "        g_space = self.colourspace_g(projection)\n",
    "        g_space = g_space.view((-1, 16, 1, 1))\n",
    "        g_space = self.colourspace_upscaler(g_space)\n",
    "        g_space = intermediate * g_space\n",
    "        g_space = torch.sum(g_space, dim=1, keepdim=True)\n",
    "\n",
    "        b_space = self.colourspace_b(projection)\n",
    "        b_space = b_space.view((-1, 16, 1, 1))\n",
    "        b_space = self.colourspace_upscaler(b_space)\n",
    "        b_space = intermediate * b_space\n",
    "        b_space = torch.sum(b_space, dim=1, keepdim=True)\n",
    "\n",
    "        output = torch.cat((r_space, g_space, b_space), dim=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"An Encoder for encoding images as latent vectors.\n",
    "    Input shape: (?, 3, 96, 96)\n",
    "    Output shape: (?, latent_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device: str = \"cpu\", latent_dim: int = 8):\n",
    "        \"\"\"Initialize encoder.\n",
    "        Args:\n",
    "            device: chich GPU or CPU to use.\n",
    "            latent_dim: output dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.latent_dim = latent_dim\n",
    "        self._init_modules()\n",
    "\n",
    "    def _init_modules(self):\n",
    "        \"\"\"Initialize the modules.\"\"\"\n",
    "        down_channels = [3, 64, 128, 256, 512]\n",
    "        self.down = nn.ModuleList()\n",
    "        for i in range(len(down_channels)-1):\n",
    "            self.down.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=down_channels[i],\n",
    "                        out_channels=down_channels[i+1],\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                        bias=True,\n",
    "                        ),\n",
    "                    nn.BatchNorm2d(down_channels[i+1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.reducer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=down_channels[-1],\n",
    "                out_channels=down_channels[-2],\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=True,\n",
    "                ),\n",
    "            nn.BatchNorm2d(down_channels[-2]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "            )\n",
    "\n",
    "        up_channels = [256, 128, 64, 64, 64]\n",
    "        scale_factors = [2, 2, 2, 1]\n",
    "        self.up = nn.ModuleList()\n",
    "        for i in range(len(up_channels)-1):\n",
    "            self.up.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=up_channels[i] + down_channels[-2-i],\n",
    "                        out_channels=up_channels[i+1],\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=1,\n",
    "                        bias=True,\n",
    "                        ),\n",
    "                    nn.BatchNorm2d(up_channels[i+1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Upsample(scale_factor=scale_factors[i]),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        down_again_channels = [64+3, 64, 64, 64, 64]\n",
    "        self.down_again = nn.ModuleList()\n",
    "        for i in range(len(down_again_channels)-1):\n",
    "            self.down_again.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=down_again_channels[i],\n",
    "                    out_channels=down_again_channels[i+1],\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    bias=True,\n",
    "                    )\n",
    "                )\n",
    "            self.down_again.append(nn.BatchNorm2d(down_again_channels[i+1]))\n",
    "            self.down_again.append(nn.LeakyReLU())\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                512*6*6 + 64*6*6,\n",
    "                256,\n",
    "                bias=True,\n",
    "                ),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(\n",
    "                256,\n",
    "                128,\n",
    "                bias=True,\n",
    "                ),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(\n",
    "                128,\n",
    "                self.latent_dim,\n",
    "                bias=True,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
    "        rv = torch.randn(input_tensor.size(), device=self.device) * 0.02\n",
    "        augmented_input = input_tensor + rv\n",
    "        intermediate = augmented_input\n",
    "        intermediates = [augmented_input]\n",
    "        for module in self.down:\n",
    "            intermediate = module(intermediate)\n",
    "            intermediates.append(intermediate)\n",
    "        intermediates = intermediates[:-1][::-1]\n",
    "\n",
    "        down = intermediate.view(-1, 6*6*512)\n",
    "\n",
    "        intermediate = self.reducer(intermediate)\n",
    "\n",
    "        for index, module in enumerate(self.up):\n",
    "            intermediate = torch.cat((intermediate, intermediates[index]), 1)\n",
    "            intermediate = module(intermediate)\n",
    "\n",
    "        intermediate = torch.cat((intermediate, input_tensor), 1)\n",
    "\n",
    "        for module in self.down_again:\n",
    "            intermediate = module(intermediate)\n",
    "\n",
    "        intermediate = intermediate.view(-1, 6*6*64)\n",
    "        intermediate = torch.cat((down, intermediate), -1)\n",
    "\n",
    "        projected = self.projection(intermediate)\n",
    "\n",
    "        return projected\n",
    "\n",
    "\n",
    "class DiscriminatorImage(nn.Module):\n",
    "    \"\"\"A discriminator for discerning real from generated images.\n",
    "    Input shape: (?, 3, 96, 96)\n",
    "    Output shape: (?, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        \"\"\"Initialize the discriminator.\"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self._init_modules()\n",
    "\n",
    "    def _init_modules(self):\n",
    "        \"\"\"Initialize the modules.\"\"\"\n",
    "        down_channels = [3, 64, 128, 256, 512]\n",
    "        self.down = nn.ModuleList()\n",
    "        leaky_relu = nn.LeakyReLU()\n",
    "        for i in range(4):\n",
    "            self.down.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=down_channels[i],\n",
    "                    out_channels=down_channels[i+1],\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    bias=True,\n",
    "                    )\n",
    "                )\n",
    "            self.down.append(nn.BatchNorm2d(down_channels[i+1]))\n",
    "            self.down.append(leaky_relu)\n",
    "\n",
    "        self.classifier = nn.ModuleList()\n",
    "        self.width = down_channels[-1] * 6**2\n",
    "        self.classifier.append(nn.Linear(self.width, 1))\n",
    "        self.classifier.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
    "        rv = torch.randn(input_tensor.size(), device=self.device) * 0.02\n",
    "        intermediate = input_tensor + rv\n",
    "        for module in self.down:\n",
    "            intermediate = module(intermediate)\n",
    "            rv = torch.randn(intermediate.size(), device=self.device) * 0.02 + 1\n",
    "            intermediate *= rv\n",
    "\n",
    "        intermediate = intermediate.view(-1, self.width)\n",
    "\n",
    "        for module in self.classifier:\n",
    "            intermediate = module(intermediate)\n",
    "\n",
    "        return intermediate\n",
    "\n",
    "\n",
    "class DiscriminatorLatent(nn.Module):\n",
    "    \"\"\"A discriminator for discerning real from generated vectors.\n",
    "    Input shape: (?, latent_dim)\n",
    "    Output shape: (?, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=8, device=\"cpu\"):\n",
    "        \"\"\"Initialize the Discriminator.\"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = device\n",
    "        self._init_modules()\n",
    "\n",
    "    def _init_modules(self, depth=7, width=8):\n",
    "        \"\"\"Initialize the modules.\"\"\"\n",
    "        self.pyramid = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.pyramid.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(\n",
    "                        self.latent_dim + width*i,\n",
    "                        width,\n",
    "                        bias=True,\n",
    "                        ),\n",
    "                    nn.BatchNorm1d(width),\n",
    "                    nn.LeakyReLU(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.classifier = nn.ModuleList()\n",
    "        self.classifier.append(nn.Linear(depth*width + self.latent_dim, 1))\n",
    "        self.classifier.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
    "        last = input_tensor\n",
    "        for module in self.pyramid:\n",
    "            projection = module(last)\n",
    "            rv = torch.randn(projection.size(), device=self.device) * 0.02 + 1\n",
    "            projection *= rv\n",
    "            last = torch.cat((last, projection), -1)\n",
    "        for module in self.classifier:\n",
    "            last = module(last)\n",
    "        return last\n",
    "\n",
    "\n",
    "class AEGAN():\n",
    "    \"\"\"An Autoencoder Generative Adversarial Network for making pokemon.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, noise_fn, dataloader,\n",
    "                 batch_size=32, device='cpu'):\n",
    "        \"\"\"Initialize the AEGAN.\n",
    "        Args:\n",
    "            latent_dim: latent-space dimension. Must be divisible by 4.\n",
    "            noise_fn: function f(num: int) -> pytorch tensor, (latent vectors)\n",
    "            dataloader: a pytorch dataloader for loading images\n",
    "            batch_size: training batch size. Must match that of dataloader\n",
    "            device: cpu or CUDA\n",
    "        \"\"\"\n",
    "        assert latent_dim % 4 == 0\n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = device\n",
    "        self.noise_fn = noise_fn\n",
    "        self.dataloader = dataloader\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.criterion_gen = nn.BCELoss()\n",
    "        self.criterion_recon_image = nn.L1Loss()\n",
    "        self.criterion_recon_latent = nn.MSELoss()\n",
    "        self.target_ones = torch.ones((batch_size, 1), device=device)\n",
    "        self.target_zeros = torch.zeros((batch_size, 1), device=device)\n",
    "        self._init_generator()\n",
    "        self._init_encoder()\n",
    "        self._init_dx()\n",
    "        self._init_dz()\n",
    "\n",
    "    def _init_generator(self):\n",
    "        self.generator = Generator(latent_dim=self.latent_dim)\n",
    "        self.generator = self.generator.to(self.device)\n",
    "        self.optim_g = optim.Adam(self.generator.parameters(),\n",
    "                                  lr=2e-4, betas=(0.5, 0.999),\n",
    "                                  weight_decay=1e-8)\n",
    "\n",
    "    def _init_encoder(self):\n",
    "        self.encoder = Encoder(latent_dim=self.latent_dim, device=self.device)\n",
    "        self.encoder = self.encoder.to(self.device)\n",
    "        self.optim_e = optim.Adam(self.encoder.parameters(),\n",
    "                                  lr=2e-4, betas=(0.5, 0.999),\n",
    "                                  weight_decay=1e-8)\n",
    "\n",
    "    def _init_dx(self):\n",
    "        self.discriminator_image = DiscriminatorImage(device=self.device).to(self.device)\n",
    "        self.optim_di = optim.Adam(self.discriminator_image.parameters(),\n",
    "                                   lr=1e-4, betas=(0.5, 0.999),\n",
    "                                   weight_decay=1e-8)\n",
    "\n",
    "    def _init_dz(self):\n",
    "        self.discriminator_latent = DiscriminatorLatent(\n",
    "            latent_dim=self.latent_dim,\n",
    "            device=self.device,\n",
    "            ).to(self.device)\n",
    "        self.optim_dl = optim.Adam(self.discriminator_latent.parameters(),\n",
    "                                   lr=1e-4, betas=(0.5, 0.999),\n",
    "                                   weight_decay=1e-8)\n",
    "\n",
    "\n",
    "    def generate_samples(self, latent_vec=None, num=None):\n",
    "        \"\"\"Sample images from the generator.\n",
    "        Images are returned as a 4D tensor of values between -1 and 1.\n",
    "        Dimensions are (number, channels, height, width). Returns the tensor\n",
    "        on cpu.\n",
    "        Args:\n",
    "            latent_vec: A pytorch latent vector or None\n",
    "            num: The number of samples to generate if latent_vec is None\n",
    "        If latent_vec and num are None then use self.batch_size\n",
    "        random latent vectors.\n",
    "        \"\"\"\n",
    "        num = self.batch_size if num is None else num\n",
    "        latent_vec = self.noise_fn(num) if latent_vec is None else latent_vec\n",
    "        with torch.no_grad():\n",
    "            samples = self.generator(latent_vec)\n",
    "        samples = samples.cpu()  # move images to cpu\n",
    "        return samples\n",
    "\n",
    "    def train_step_generators(self, X):\n",
    "        \"\"\"Train the generator one step and return the loss.\"\"\"\n",
    "        self.generator.zero_grad()\n",
    "        self.encoder.zero_grad()\n",
    "\n",
    "        Z = self.noise_fn(self.batch_size)\n",
    "\n",
    "        X_hat = self.generator(Z)\n",
    "        Z_hat = self.encoder(X)\n",
    "        X_tilde = self.generator(Z_hat)\n",
    "        Z_tilde = self.encoder(X_hat)\n",
    "\n",
    "        X_hat_confidence = self.discriminator_image(X_hat)\n",
    "        Z_hat_confidence = self.discriminator_latent(Z_hat)\n",
    "        X_tilde_confidence = self.discriminator_image(X_tilde)\n",
    "        Z_tilde_confidence = self.discriminator_latent(Z_tilde)\n",
    "\n",
    "        X_hat_loss = self.criterion_gen(X_hat_confidence, self.target_ones)\n",
    "        Z_hat_loss = self.criterion_gen(Z_hat_confidence, self.target_ones)\n",
    "        X_tilde_loss = self.criterion_gen(X_tilde_confidence, self.target_ones)\n",
    "        Z_tilde_loss = self.criterion_gen(Z_tilde_confidence, self.target_ones)\n",
    "\n",
    "        X_recon_loss = self.criterion_recon_image(X_tilde, X) * ALPHA_RECONSTRUCT_IMAGE\n",
    "        Z_recon_loss = self.criterion_recon_latent(Z_tilde, Z) * ALPHA_RECONSTRUCT_LATENT\n",
    "\n",
    "        X_loss = (X_hat_loss + X_tilde_loss) / 2 * ALPHA_DISCRIMINATE_IMAGE\n",
    "        Z_loss = (Z_hat_loss + Z_tilde_loss) / 2 * ALPHA_DISCRIMINATE_LATENT\n",
    "        loss = X_loss + Z_loss + X_recon_loss + Z_recon_loss\n",
    "\n",
    "        loss.backward()\n",
    "        self.optim_e.step()\n",
    "        self.optim_g.step()\n",
    "\n",
    "        return X_loss.item(), Z_loss.item(), X_recon_loss.item(), Z_recon_loss.item()\n",
    "\n",
    "    def train_step_discriminators(self, X):\n",
    "        \"\"\"Train the discriminator one step and return the losses.\"\"\"\n",
    "        self.discriminator_image.zero_grad()\n",
    "        self.discriminator_latent.zero_grad()\n",
    "\n",
    "        Z = self.noise_fn(self.batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X_hat = self.generator(Z)\n",
    "            Z_hat = self.encoder(X)\n",
    "            X_tilde = self.generator(Z_hat)\n",
    "            Z_tilde = self.encoder(X_hat)\n",
    "\n",
    "        X_confidence = self.discriminator_image(X)\n",
    "        X_hat_confidence = self.discriminator_image(X_hat)\n",
    "        X_tilde_confidence = self.discriminator_image(X_tilde)\n",
    "        Z_confidence = self.discriminator_latent(Z)\n",
    "        Z_hat_confidence = self.discriminator_latent(Z_hat)\n",
    "        Z_tilde_confidence = self.discriminator_latent(Z_tilde)\n",
    "\n",
    "        X_loss = 2 * self.criterion_gen(X_confidence, self.target_ones)\n",
    "        X_hat_loss = self.criterion_gen(X_hat_confidence, self.target_zeros)\n",
    "        X_tilde_loss = self.criterion_gen(X_tilde_confidence, self.target_zeros)\n",
    "        Z_loss = 2 * self.criterion_gen(Z_confidence, self.target_ones)\n",
    "        Z_hat_loss = self.criterion_gen(Z_hat_confidence, self.target_zeros)\n",
    "        Z_tilde_loss = self.criterion_gen(Z_tilde_confidence, self.target_zeros)\n",
    "\n",
    "        loss_images = (X_loss + X_hat_loss + X_tilde_loss) / 4\n",
    "        loss_latent = (Z_loss + Z_hat_loss + Z_tilde_loss) / 4\n",
    "        loss = loss_images + loss_latent\n",
    "\n",
    "        loss.backward()\n",
    "        self.optim_di.step()\n",
    "        self.optim_dl.step()\n",
    "\n",
    "        return loss_images.item(), loss_latent.item()\n",
    "\n",
    "    def train_epoch(self, print_frequency=1, max_steps=0):\n",
    "        \"\"\"Train both networks for one epoch and return the losses.\n",
    "        Args:\n",
    "            print_frequency (int): print stats every `print_frequency` steps.\n",
    "            max_steps (int): End epoch after `max_steps` steps, or set to 0\n",
    "                             to do the full epoch.\n",
    "        \"\"\"\n",
    "        ldx, ldz, lgx, lgz, lrx, lrz = 0, 0, 0, 0, 0, 0\n",
    "        eps = 1e-9\n",
    "        for batch, (real_samples, _) in enumerate(self.dataloader):\n",
    "            real_samples = real_samples.to(self.device)\n",
    "            ldx_, ldz_ = self.train_step_discriminators(real_samples)\n",
    "            ldx += ldx_\n",
    "            ldz += ldz_\n",
    "            lgx_, lgz_, lrx_, lrz_ = self.train_step_generators(real_samples)\n",
    "            lgx += lgx_\n",
    "            lgz += lgz_\n",
    "            lrx += lrx_\n",
    "            lrz += lrz_\n",
    "            if print_frequency and (batch+1) % print_frequency == 0:\n",
    "                print(f\"{batch+1}/{len(self.dataloader)}:\"\n",
    "                      f\" G={lgx / (eps + (batch+1) * ALPHA_DISCRIMINATE_IMAGE):.3f},\"\n",
    "                      f\" E={lgz / (eps + (batch+1) * ALPHA_DISCRIMINATE_LATENT):.3f},\"\n",
    "                      f\" Dx={ldx / (eps + (batch+1)):.3f},\"\n",
    "                      f\" Dz={ldz / (eps + (batch+1)):.3f}\",\n",
    "                      f\" Rx={lrx / (eps + (batch+1) * ALPHA_RECONSTRUCT_IMAGE):.3f}\",\n",
    "                      f\" Rz={lrz / (eps + (batch+1) * ALPHA_RECONSTRUCT_LATENT):.3f}\",\n",
    "                      end='\\r',\n",
    "                      flush=True)\n",
    "            if max_steps and batch == max_steps:\n",
    "                break\n",
    "        if print_frequency:\n",
    "            print()\n",
    "        lgx /= batch\n",
    "        lgz /= batch\n",
    "        ldx /= batch\n",
    "        ldz /= batch\n",
    "        lrx /= batch\n",
    "        lrz /= batch\n",
    "        return lgx, lgz, ldx, ldz, lrx, lrz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
